---
layout:     post
title:      深度学习三
subtitle:   基于Python的理论和实现
date:       2026-02-13
author:     LXG
header-img: img/post-bg-nvida.jpg
catalog: true
tags:
    - AI
---

## 误差反向传播法

**前向传播 + 反向传播对比计算量的示意图**

![backpropagation](/images/ai/backpropagation.png)

**用迷宫来理解反向传播**

![backpropagation_2](/images/ai/backpropagation_2.png)

**计算图**

* 正向传播(forward propagation) = 预测结果
* 反向传播(backward propagation) = 分析错误原因，告诉每个神经元怎么调整

**链式法则**

* 前向传播中链式法则： 就像流水线上每个工序，把材料一步步加工成半成品 → 最终得到成品。你只关心材料怎么经过每一层，最终变成成品，不关心每个工序对最终误差的贡献。
* 反向传播中链式法则： 链式法则就像在流水线末端发现问题（成品有瑕疵），你要沿传送带回头追溯每个工序对这个问题的影响，然后告诉每台机器该如何调整。你需要知道每一层“对最终误差的贡献”，才能高效更新参数。

![backpropagation_3](/images/ai/backpropagation_3.png)

**比喻**

* 前向传播 = 你每个机器都单独调试一次 → 每次都要重新运行整个流水线
* 反向传播 = 你先让流水线走一次，把每台机器的中间状态记录下来 → 回溯计算每台机器对最终产品影响 → 不用重复跑整条线

## GPU 进入深度学习的时间表

**NVIDIA GPU 开始进入深度学习领域**

1986年 Rumelhart 等人系统化提出“多层前馈神经网络的误差反向传播训练算法”，并演示它在实际任务中有效，才让反向传播真正成为神经网络训练的核心方法

| 年份   | 事件                    | 备注                                  |
| ---- | --------------------- | ----------------------------------- |
| 2006 | 深度信念网络提出              | Hinton 等人，推动深度学习复兴                  |
| 2007 | CUDA 平台发布             | NVIDIA 开放 GPU 并行计算能力                |
| 2009 | 深度学习开始用 GPU 训练        | Hinton 团队训练 MNIST、ImageNet 子集       |
| 2012 | AlexNet 获 ImageNet 冠军 | 使用 NVIDIA GPU 训练 CNN，GPU 成为深度学习训练标准 |

**GPU厂商进入深度学习领域时间对比**

| 厂商                                 | 平台 / 技术                      | 进入深度学习应用时间                        | 备注 / 特点                                        |
| ---------------------------------- | ---------------------------- | --------------------------------- | ---------------------------------------------- |
| NVIDIA                             | CUDA                         | 2007（平台发布），2009–2010（研究者开始训练深度网络） | 第一个成熟并行计算工具，生态完善，GPU训练深度网络主流                   |
| AMD                                | Stream SDK / OpenCL          | 2009–2012                         | 工具和生态不如 CUDA 成熟，少数研究团队尝试训练神经网络                 |
| Intel                              | CPU / Xeon Phi / FPGA        | 2007–2012                         | 以 CPU 为主，Xeon Phi 用于并行计算，FPGA 做实验性加速，GPU通用计算滞后 |
| IBM                                | Cell/Broadband Engine        | 2008–2011                         | PS3 GPU/Cell 架构用于小型神经网络实验，实际大规模训练受限            |
| ARM                                | Mali GPU                     | 2010–2013                         | 移动端低功耗 GPU，适合轻量级网络或嵌入式推理，训练能力有限                |
| Google                             | TPU (Tensor Processing Unit) | 2016                              | 专用深度学习加速器，面向训练和推理，非通用 GPU，但极大提升大规模训练效率         |
| FPGA 厂商                           | OpenCL / 自定义加速            | 2012–2015                         | 用于实验性神经网络加速，灵活但开发复杂，生态不成熟                      |
























