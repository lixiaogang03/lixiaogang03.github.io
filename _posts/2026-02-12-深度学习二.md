---
layout:     post
title:      æ·±åº¦å­¦ä¹ äºŒ
subtitle:   åŸºäºPythonçš„ç†è®ºå’Œå®ç°
date:       2026-02-12
author:     LXG
header-img: img/post-bg-universe.jpg
catalog: true
tags:
    - Tool
    - AI
---

[2026 Agent ç¼–ç¨‹è¶‹åŠ¿æŠ¥å‘Š-Anthropic](https://baoyu.io/blog/2026/02/09/anthropic-agentic-coding-trends-2026)

## ä¸¤å±‚ç¥ç»ç½‘ç»œ 

```py

import sys, os

import numpy as np

# sigmoid å‡½æ•°ï¼šæŠŠä»»æ„æ•°å€¼å‹ç¼©åˆ° 0-1 ä¹‹é—´ï¼ˆç”¨äºéšè—å±‚ï¼‰
def sigmoid(x):
    return 1 / (1 + np.exp(-x))    


def sigmoid_grad(x):
    """sigmoid å‡½æ•°çš„å¯¼æ•°ï¼šç”¨äºåå‘ä¼ æ’­æ—¶è®¡ç®—æ¢¯åº¦
    
    æ•°å­¦å½¢å¼ï¼šd(sigmoid)/dx = sigmoid(x) * (1 - sigmoid(x))
    """
    return (1.0 - sigmoid(x)) * sigmoid(x)

# softmax å‡½æ•°ï¼šæŠŠä»»æ„æ•°å€¼è½¬æ¢æˆ 0-1 ä¹‹é—´çš„æ¦‚ç‡ï¼ˆç”¨äºè¾“å‡ºå±‚ï¼‰
def softmax(x):
    if x.ndim == 2:
        x = x.T
        x = x - np.max(x, axis=0)
        y = np.exp(x) / np.sum(np.exp(x), axis=0)
        return y.T 

    x = x - np.max(x) # å‡å»æœ€å¤§å€¼ï¼Œé˜²æ­¢æ•°å€¼æº¢å‡ºï¼ˆè®¡ç®—ç¨³å®šæ€§æŠ€å·§ï¼‰
    return np.exp(x) / np.sum(np.exp(x)) # æŒ‡æ•°å½’ä¸€åŒ–æˆæ¦‚ç‡

def cross_entropy_error(y, t):
    """äº¤å‰ç†µæŸå¤±å‡½æ•°ï¼šè¡¡é‡æ¨¡å‹é¢„æµ‹å’ŒçœŸå®æ ‡ç­¾çš„å·®è·
    
    å‚æ•°
    -----
    y : numpy.ndarray
        æ¨¡å‹é¢„æµ‹çš„æ¦‚ç‡åˆ†å¸ƒï¼ˆç”± softmax è¾“å‡ºï¼‰
    t : numpy.ndarray
        çœŸå®æ ‡ç­¾ï¼ˆç‹¬çƒ­ç¼–ç æˆ–ç±»åˆ«ç´¢å¼•ï¼‰
    
    è¿”å›
    ----
    float
        äº¤å‰ç†µæŸå¤±ï¼ˆæ•°å€¼è¶Šå°è¡¨ç¤ºé¢„æµ‹è¶Šå‡†ç¡®ï¼‰
    """
    if y.ndim == 1:
        t = t.reshape(1, t.size)
        y = y.reshape(1, y.size)
        
    # å¦‚æœæ ‡ç­¾æ˜¯ç‹¬çƒ­ç¼–ç ï¼ˆå¦‚ [0, 1, 0]ï¼‰ï¼Œè½¬æ¢æˆç±»åˆ«ç´¢å¼•ï¼ˆå¦‚ 1ï¼‰
    if t.size == y.size:
        t = t.argmax(axis=1)
             
    batch_size = y.shape[0]
    # è®¡ç®—å¹³å‡çš„äº¤å‰ç†µï¼ˆåŠ ä¸ªå¾ˆå°çš„å€¼ 1e-7 é˜²æ­¢å–å¯¹æ•°æ—¶å‡ºç°è´Ÿæ— ç©·ï¼‰
    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size

def numerical_gradient(f, x):
    """æ•°å€¼æ¢¯åº¦è®¡ç®—å‡½æ•°ï¼šç”¨æœ‰é™å·®åˆ†æ³•è¿‘ä¼¼è®¡ç®—å‡½æ•° f å¯¹ x çš„æ¢¯åº¦
    
    åŸç†ï¼šæ¢¯åº¦ â‰ˆ (f(x+h) - f(x-h)) / (2*h)ï¼Œå…¶ä¸­ h æ˜¯ä¸€ä¸ªå¾ˆå°çš„æ•°
    
    å‚æ•°
    -----
    f : callable
        ç›®æ ‡å‡½æ•°ï¼ˆæ¥å— x è¿”å›æ ‡é‡ï¼‰
    x : numpy.ndarray
        è¾“å…¥å˜é‡ï¼ˆå¯ä»¥æ˜¯å‘é‡æˆ–çŸ©é˜µï¼‰
    
    è¿”å›
    ----
    grad : numpy.ndarray
        æ¢¯åº¦ï¼Œä¸ x å½¢çŠ¶ç›¸åŒ
    """
    h = 1e-4 # å¾ˆå°çš„æ­¥é•¿ï¼ˆ0.0001ï¼‰ï¼Œç”¨æ¥è¿‘ä¼¼å¯¼æ•°
    grad = np.zeros_like(x)  # åˆå§‹åŒ–æ¢¯åº¦æ•°ç»„
    
    # éå† x çš„æ¯ä¸ªå…ƒç´ 
    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])
    while not it.finished:
        idx = it.multi_index  # è·å–å½“å‰å…ƒç´ çš„ç´¢å¼•
        tmp_val = x[idx]  # ä¿å­˜åŸå§‹å€¼
        
        # è®¡ç®— f(x+h)ï¼šç¬¬ idx ä¸ªå…ƒç´ å¢åŠ  h
        x[idx] = float(tmp_val) + h
        fxh1 = f(x)
        
        # è®¡ç®— f(x-h)ï¼šç¬¬ idx ä¸ªå…ƒç´ å‡å°‘ h
        x[idx] = tmp_val - h 
        fxh2 = f(x)
        
        # ç”¨ä¸­å¿ƒå·®åˆ†å…¬å¼è®¡ç®—æ¢¯åº¦ï¼ˆåå¯¼æ•°ï¼‰
        grad[idx] = (fxh1 - fxh2) / (2*h)
        
        # æ¢å¤åŸå§‹å€¼ï¼Œå‡†å¤‡è®¡ç®—ä¸‹ä¸€ä¸ªå…ƒç´ 
        x[idx] = tmp_val
        it.iternext()   
        
    return grad

class TwoLayerNet:
    """ä¸¤å±‚ç¥ç»ç½‘ç»œç±»ï¼šè¾“å…¥ -> éšè—å±‚ -> è¾“å‡ºå±‚
    
    è¿™æ˜¯ä¸€ä¸ªç®€å•çš„å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰ï¼Œç”¨ sigmoid æ¿€æ´»éšè—å±‚ï¼Œsoftmax æ¿€æ´»è¾“å‡ºå±‚ã€‚
    """
    
    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):
        """åˆå§‹åŒ–ç½‘ç»œçš„æƒé‡å’Œåç½®ï¼ˆç”¨éšæœºæ•°å¡«å……ï¼‰ã€‚
        
        å‚æ•°
        -----
        input_size : è¾“å…¥èŠ‚ç‚¹ä¸ªæ•°
        hidden_size : éšè—å±‚èŠ‚ç‚¹ä¸ªæ•°
        output_size : è¾“å‡ºèŠ‚ç‚¹ä¸ªæ•°
        weight_init_std : æƒé‡åˆå§‹åŒ–çš„éšæœºæ•°å¤§å°ï¼ˆè¾ƒå°çš„æ•°æœ‰åˆ©äºè®­ç»ƒï¼‰
        """
        # åˆ›å»ºç½‘ç»œå‚æ•°å­—å…¸ï¼Œå­˜æ”¾æ‰€æœ‰æƒé‡å’Œåç½®
        self.params = {}
        # ç¬¬ä¸€å±‚ï¼ˆè¾“å…¥åˆ°éšè—å±‚ï¼‰çš„æƒé‡å’Œåç½®
        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)
        self.params['b1'] = np.zeros(hidden_size)
        # ç¬¬äºŒå±‚ï¼ˆéšè—å±‚åˆ°è¾“å‡ºå±‚ï¼‰çš„æƒé‡å’Œåç½®
        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)
        self.params['b2'] = np.zeros(output_size)

    def predict(self, x):
        """å‰å‘ä¼ æ’­ï¼šè¾“å…¥æ•°æ®é€šè¿‡ç½‘ç»œå¾—åˆ°é¢„æµ‹ç»“æœã€‚
        
        è®¡ç®—è¿‡ç¨‹ï¼šè¾“å…¥ -> (W1, b1) -> sigmoid -> (W2, b2) -> softmax -> è¾“å‡ºæ¦‚ç‡
        """
        W1, W2 = self.params['W1'], self.params['W2']
        b1, b2 = self.params['b1'], self.params['b2']
    
        # ç¬¬ä¸€å±‚ï¼šè¾“å…¥ dot æƒé‡ + åç½®ï¼Œç„¶åé€šè¿‡ sigmoid æ¿€æ´»
        a1 = np.dot(x, W1) + b1
        z1 = sigmoid(a1)
        
        # ç¬¬äºŒå±‚ï¼šéšè—å±‚è¾“å‡º dot æƒé‡ + åç½®ï¼Œç„¶åé€šè¿‡ softmax æ¿€æ´»å¾—åˆ°æ¦‚ç‡
        a2 = np.dot(z1, W2) + b2
        y = softmax(a2)
        
        return y

    def loss(self, x, t):
        """è®¡ç®—æŸå¤±å‡½æ•°å€¼ï¼šè¡¡é‡ç½‘ç»œé¢„æµ‹ä¸çœŸå®æ ‡ç­¾çš„è¯¯å·®
        
        å‚æ•°
        -----
        x : numpy.ndarray
            è¾“å…¥æ•°æ®ï¼Œå½¢çŠ¶ (æ ·æœ¬æ•°, è¾“å…¥ç»´åº¦)
        t : numpy.ndarray
            çœŸå®æ ‡ç­¾ï¼Œå½¢çŠ¶ (æ ·æœ¬æ•°, è¾“å‡ºç»´åº¦) æˆ– (æ ·æœ¬æ•°,)
        
        è¿”å›
        ----
        float
            äº¤å‰ç†µæŸå¤±å€¼
        """
        y = self.predict(x)  # å‰å‘ä¼ æ’­å¾—åˆ°é¢„æµ‹ç»“æœ
        
        return cross_entropy_error(y, t)  # è®¡ç®—æŸå¤±

    def accuracy(self, x, t):
        """è®¡ç®—ç½‘ç»œçš„å‡†ç¡®ç‡ï¼šé¢„æµ‹æ­£ç¡®çš„æ ·æœ¬æ•°é‡å æ¯”
        
        å‚æ•°
        -----
        x : numpy.ndarray
            è¾“å…¥æ•°æ®ï¼Œå½¢çŠ¶ (æ ·æœ¬æ•°, è¾“å…¥ç»´åº¦)
        t : numpy.ndarray
            çœŸå®æ ‡ç­¾ï¼Œå½¢çŠ¶ (æ ·æœ¬æ•°, è¾“å‡ºç»´åº¦)ï¼ˆé€šå¸¸æ˜¯ç‹¬çƒ­ç¼–ç ï¼‰
        
        è¿”å›
        ----
        float
            å‡†ç¡®ç‡ï¼ŒèŒƒå›´ 0-1ï¼ˆ0 = å…¨é”™ï¼Œ1 = å…¨å¯¹ï¼‰
        """
        y = self.predict(x)  # å‰å‘ä¼ æ’­å¾—åˆ°é¢„æµ‹æ¦‚ç‡
        y = np.argmax(y, axis=1)  # é€‰æ‹©æ¦‚ç‡æœ€å¤§çš„ç±»åˆ«ä½œä¸ºé¢„æµ‹ç»“æœ
        t = np.argmax(t, axis=1)  # ä»ç‹¬çƒ­ç¼–ç è½¬æ¢ä¸ºç±»åˆ«ç´¢å¼•
        
        # æ¯”è¾ƒé¢„æµ‹å’ŒçœŸå®æ ‡ç­¾ï¼Œè®¡ç®—ç›¸ç­‰çš„æ•°é‡å æ¯”
        accuracy = np.sum(y == t) / float(x.shape[0])
        return accuracy
            
    def numerical_gradient(self, x, t):
        """ç”¨æ•°å€¼å¾®åˆ†æ³•è®¡ç®—æ‰€æœ‰å‚æ•°çš„æ¢¯åº¦ï¼ˆé€Ÿåº¦æ…¢ï¼Œä½†æ˜“äºç†è§£å’ŒéªŒè¯ï¼‰
        
        å‚æ•°
        -----
        x : numpy.ndarray
            è¾“å…¥æ•°æ®ï¼Œå½¢çŠ¶ (æ ·æœ¬æ•°, è¾“å…¥ç»´åº¦)
        t : numpy.ndarray
            çœŸå®æ ‡ç­¾ï¼Œå½¢çŠ¶ (æ ·æœ¬æ•°, è¾“å‡ºç»´åº¦)
        
        è¿”å›
        ----
        grads : dict
            æ¢¯åº¦å­—å…¸ï¼ŒåŒ…å« 'W1', 'b1', 'W2', 'b2' å››ä¸ªæ¢¯åº¦å€¼
        """
        # å®šä¹‰ä¸€ä¸ªå…³äºæƒé‡ W çš„æŸå¤±å‡½æ•°ï¼ˆç”¨äºæ•°å€¼å¾®åˆ†ï¼‰
        loss_W = lambda W: self.loss(x, t)
        
        # è®¡ç®—æ¯ä¸ªå‚æ•°çš„æ¢¯åº¦
        grads = {}
        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])
        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])
        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])
        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])
        
        return grads
        
    def gradient(self, x, t):
        """ç”¨åå‘ä¼ æ’­ç®—æ³•è®¡ç®—æ‰€æœ‰å‚æ•°çš„æ¢¯åº¦ï¼ˆæ¯”æ•°å€¼å¾®åˆ†å¿«å¾—å¤šï¼‰
        
        è¿™æ˜¯æ·±åº¦å­¦ä¹ ä¸­æœ€å¸¸ç”¨çš„æ¢¯åº¦è®¡ç®—æ–¹æ³•ï¼Œé€šè¿‡é“¾å¼æ³•åˆ™ä»è¾“å‡ºå±‚åå‘è®¡ç®—æ¯å±‚çš„æ¢¯åº¦ã€‚
        
        å‚æ•°
        -----
        x : numpy.ndarray
            è¾“å…¥æ•°æ®ï¼Œå½¢çŠ¶ (æ ·æœ¬æ•°, è¾“å…¥ç»´åº¦)
        t : numpy.ndarray
            çœŸå®æ ‡ç­¾ï¼Œå½¢çŠ¶ (æ ·æœ¬æ•°, è¾“å‡ºç»´åº¦)
        
        è¿”å›
        ----
        grads : dict
            æ¢¯åº¦å­—å…¸ï¼ŒåŒ…å« 'W1', 'b1', 'W2', 'b2' å››ä¸ªæ¢¯åº¦å€¼
        """
        W1, W2 = self.params['W1'], self.params['W2']
        b1, b2 = self.params['b1'], self.params['b2']
        grads = {}
        
        batch_num = x.shape[0]  # æ‰¹æ¬¡ä¸­æ ·æœ¬çš„ä¸ªæ•°
        
        # ========== å‰å‘ä¼ æ’­ï¼šè®¡ç®—ç½‘ç»œè¾“å‡º ==========
        a1 = np.dot(x, W1) + b1  # ç¬¬ä¸€å±‚çš„çº¿æ€§å˜æ¢
        z1 = sigmoid(a1)  # ç¬¬ä¸€å±‚çš„æ¿€æ´»å‡½æ•°
        a2 = np.dot(z1, W2) + b2  # ç¬¬äºŒå±‚çš„çº¿æ€§å˜æ¢
        y = softmax(a2)  # ç¬¬äºŒå±‚çš„æ¿€æ´»å‡½æ•°ï¼ˆè¾“å‡ºæ¦‚ç‡ï¼‰
        
        # ========== åå‘ä¼ æ’­ï¼šä»è¾“å‡ºå±‚å¼€å§‹åå‘è®¡ç®—æ¢¯åº¦ ==========
        # ç¬¬äºŒå±‚çš„æ¢¯åº¦ï¼š(é¢„æµ‹æ¦‚ç‡ - çœŸå®æ ‡ç­¾) / æ ·æœ¬æ•°ï¼ˆè¿™æ¥è‡ªäº¤å‰ç†µ+ softmax çš„æ±‚å¯¼ï¼‰
        dy = (y - t) / batch_num
        # è®¡ç®—ç¬¬äºŒå±‚å‚æ•°çš„æ¢¯åº¦
        grads['W2'] = np.dot(z1.T, dy)  # éšè—å±‚æ¿€æ´»çš„è½¬ç½® dot è¾“å‡ºè¯¯å·®
        grads['b2'] = np.sum(dy, axis=0)  # æ²¿æ ·æœ¬ç»´åº¦æ±‚å’Œå¾—åˆ°åç½®æ¢¯åº¦
        
        # ç¬¬ä¸€å±‚çš„æ¢¯åº¦ï¼šåå‘ä¼ æ’­è¯¯å·®
        da1 = np.dot(dy, W2.T)  # è¯¯å·®é€šè¿‡æƒé‡ W2 åå‘ä¼ æ’­
        dz1 = sigmoid_grad(a1) * da1  # ä¹˜ä»¥ sigmoid å¯¼æ•°ï¼ˆé“¾å¼æ³•åˆ™ï¼‰
        # è®¡ç®—ç¬¬ä¸€å±‚å‚æ•°çš„æ¢¯åº¦
        grads['W1'] = np.dot(x.T, dz1)  # è¾“å…¥çš„è½¬ç½® dot ç¬¬ä¸€å±‚è¯¯å·®
        grads['b1'] = np.sum(dz1, axis=0)  # æ²¿æ ·æœ¬ç»´åº¦æ±‚å’Œå¾—åˆ°åç½®æ¢¯åº¦

        return grads

```

### å‡½æ•°è§£é‡Š  

| å‡½æ•°å | åŠŸèƒ½æè¿° | ç”¨é€” |
|--------|--------|------|
| `sigmoid(x)` | å°†ä»»æ„æ•°å€¼å‹ç¼©åˆ° 0-1 ä¹‹é—´ | éšè—å±‚æ¿€æ´»å‡½æ•° |
| `sigmoid_grad(x)` | è®¡ç®— sigmoid çš„å¯¼æ•°å€¼ | åå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦ |
| `softmax(x)` | å°†ä»»æ„æ•°å€¼è½¬æ¢ä¸º 0-1 çš„æ¦‚ç‡åˆ†å¸ƒ | è¾“å‡ºå±‚æ¿€æ´»å‡½æ•° |
| `cross_entropy_error(y, t)` | è®¡ç®—æ¨¡å‹é¢„æµ‹ä¸çœŸå®æ ‡ç­¾çš„è¯¯å·® | æŸå¤±å‡½æ•° |
| `numerical_gradient(f, x)` | ä½¿ç”¨æœ‰é™å·®åˆ†æ³•è®¡ç®—å‡½æ•°æ¢¯åº¦ | éªŒè¯åå‘ä¼ æ’­ç®—æ³•æ­£ç¡®æ€§ |
| `TwoLayerNet.__init__()` | åˆå§‹åŒ–ç½‘ç»œçš„æƒé‡å’Œåç½® | ç½‘ç»œå‚æ•°åˆå§‹åŒ– |
| `TwoLayerNet.predict(x)` | å‰å‘ä¼ æ’­è®¡ç®—ç½‘ç»œé¢„æµ‹å€¼ | é¢„æµ‹è¾“å‡º |
| `TwoLayerNet.loss(x, t)` | è®¡ç®—äº¤å‰ç†µæŸå¤±å‡½æ•°å€¼ | è¯„ä¼°è®­ç»ƒæ•ˆæœ |
| `TwoLayerNet.accuracy(x, t)` | è®¡ç®—åˆ†ç±»å‡†ç¡®ç‡ | è¯„ä¼°æ¨¡å‹æ€§èƒ½ |
| `TwoLayerNet.numerical_gradient(x, t)` | ç”¨æ•°å€¼å¾®åˆ†æ³•è®¡ç®—æ¢¯åº¦ | æ¢¯åº¦éªŒè¯ç”¨ |
| `TwoLayerNet.gradient(x, t)` | ç”¨åå‘ä¼ æ’­ç®—æ³•è®¡ç®—æ¢¯åº¦ | è®­ç»ƒç½‘ç»œç”¨ï¼ˆæ¨èï¼‰ |

### è®­ç»ƒæµç¨‹

å‰å‘ä¼ æ’­ â†’ è®¡ç®—æŸå¤± â†’ åå‘ä¼ æ’­ â†’ æ›´æ–°å‚æ•° â†’ å¾ªç¯

### æ¢¯åº¦è®¡ç®—æ–¹æ³•å¯¹æ¯”

| æ¢¯åº¦è®¡ç®—æ–¹æ³• | è®¡ç®—é€Ÿåº¦ | æ˜“ç”¨æ€§ | åº”ç”¨åœºæ™¯ |
|-------------|--------|-------|--------|
| æ•°å€¼å¾®åˆ† | ğŸ”´ å¾ˆæ…¢ | ğŸŸ¢ æ˜“äºç†è§£ | éªŒè¯å’Œå­¦ä¹ ç”¨ |
| åå‘ä¼ æ’­ | ğŸŸ¢ å¾ˆå¿« | ğŸŸ¡ éœ€è¦æ•°å­¦åŸºç¡€ | å®é™…è®­ç»ƒç”¨ |

### MNIST æ•°æ®è®­ç»ƒ

```py

import numpy as np
from mnist_demo import load_mnist
from two_layer import TwoLayerNet

# åŠ è½½ MNIST æ‰‹å†™æ•°å­—æ•°æ®é›†
# normalize=Trueï¼šåƒç´ å€¼ç¼©æ”¾åˆ° 0-1ï¼ˆä¾¿äºç¥ç»ç½‘ç»œå¤„ç†ï¼‰
# flatten=Trueï¼šæŠŠ 28x28 çš„å›¾ç‰‡å±•å¹³æˆ 784 ç»´å‘é‡
# one_hot_label=Trueï¼šæ ‡ç­¾è½¬æ¢æˆç‹¬çƒ­ç¼–ç ï¼ˆä¾‹å¦‚ 5 å˜æˆ [0,0,0,0,0,1,0,0,0,0]ï¼‰
(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, flatten=True, one_hot_label=True)

train_loss_list = []  # å­˜å‚¨æ¯æ¬¡è¿­ä»£çš„æŸå¤±å€¼ï¼Œç”¨äºç”»å›¾è§‚å¯Ÿè®­ç»ƒè¿‡ç¨‹

# ========== è®­ç»ƒè¶…å‚æ•°è®¾ç½® ==========
# è¿™äº›æ˜¯ç¥ç»ç½‘ç»œè®­ç»ƒæ—¶éœ€è¦æ‰‹åŠ¨è°ƒæ•´çš„å‚æ•°
iters_num = 10000  # æ€»å…±è®­ç»ƒå¤šå°‘æ­¥ï¼ˆæ¯æ­¥å¤„ç†ä¸€ä¸ªå°æ‰¹é‡æ•°æ®ï¼‰
train_size = x_train.shape[0]  # è®­ç»ƒé›†ä¸­æœ‰å¤šå°‘å¼ å›¾ç‰‡
batch_size = 100  # æ¯æ¬¡è®­ç»ƒç”¨å¤šå°‘å¼ å›¾ç‰‡ï¼ˆ100 å¼ ä¸ºä¸€ä¸ªæ‰¹æ¬¡ï¼‰
learning_rate = 0.1  # å­¦ä¹ ç‡ï¼šæ§åˆ¶æ¯æ¬¡å‚æ•°æ›´æ–°çš„å¹…åº¦ï¼ˆæ•°å€¼è¶Šå¤§å­¦ä¹ è¶Šå¿«ï¼Œä½†å¯èƒ½è·³è¿‡æœ€ä¼˜å€¼ï¼‰

# åˆå§‹åŒ–ç¥ç»ç½‘ç»œï¼š784 ä¸ªè¾“å…¥ -> 100 ä¸ªéšè—å±‚èŠ‚ç‚¹ -> 10 ä¸ªè¾“å‡ºï¼ˆ0-9 åä¸ªæ•°å­—ï¼‰
network = TwoLayerNet(input_size=784, hidden_size=100, output_size=10)

# ========== å¼€å§‹è®­ç»ƒå¾ªç¯ ==========
for i in range(iters_num):
    # ä»è®­ç»ƒé›†ä¸­éšæœºæŠ½å– batch_size å¼ å›¾ç‰‡ï¼ˆå°æ‰¹é‡éšæœºæŠ½æ ·ï¼Œè¿™æ ·è®­ç»ƒæ›´ç¨³å®šï¼‰
    batch_mask = np.random.choice(train_size, batch_size)
    x_batch = x_train[batch_mask]  # æŠ½å–çš„å›¾ç‰‡æ•°æ®
    t_batch = t_train[batch_mask]  # æŠ½å–çš„å›¾ç‰‡æ ‡ç­¾

    # è®¡ç®—æ¢¯åº¦ï¼šæ¢¯åº¦å‘Šè¯‰æˆ‘ä»¬å‚æ•°åº”è¯¥å¾€å“ªä¸ªæ–¹å‘è°ƒæ•´æ‰èƒ½å‡å°æŸå¤±
    grad = network.numerical_gradient(x_batch, t_batch)
    # grad = network.gradient(x_batch, t_batch)  # å¯ä»¥ç”¨æ›´å¿«çš„åå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦

    # å‚æ•°æ›´æ–°ï¼šæ²¿ç€æ¢¯åº¦çš„åæ–¹å‘ï¼ˆä¸‹é™æ–¹å‘ï¼‰è°ƒæ•´å‚æ•°
    # æ–°å‚æ•° = æ—§å‚æ•° - å­¦ä¹ ç‡ Ã— æ¢¯åº¦
    for key in ('W1', 'b1', 'W2', 'b2'):
        network.params[key] -= learning_rate * grad[key]

    # è®¡ç®—å½“å‰å°æ‰¹é‡çš„æŸå¤±å€¼ï¼ˆè¡¡é‡é¢„æµ‹æœ‰å¤šé”™è¯¯ï¼‰
    loss = network.loss(x_batch, t_batch)
    train_loss_list.append(loss)  # è®°å½•è¿™ä¸€æ­¥çš„æŸå¤±å€¼

    # æ¯ 1000 æ­¥æ‰“å°ä¸€æ¬¡è¿›åº¦å’ŒæŸå¤±å€¼
    if i % 1000 == 0:
        print(f"è®­ç»ƒè¿›åº¦: ç¬¬ {i} æ­¥ï¼Œå½“å‰æŸå¤±å€¼: {loss:.4f}")


# ========== è®­ç»ƒå®Œæˆï¼Œç»˜åˆ¶æŸå¤±å‡½æ•°æ›²çº¿ ==========
import matplotlib.pyplot as plt

# åˆ›å»ºå›¾è¡¨
plt.figure(figsize=(12, 6))

# ç»˜åˆ¶æŸå¤±å‡½æ•°å€¼æ¨ç§»æ›²çº¿
plt.plot(train_loss_list, linewidth=0.5)

# è®¾ç½®å›¾è¡¨æ ‡é¢˜å’Œæ ‡ç­¾
plt.title("è®­ç»ƒè¿‡ç¨‹ä¸­çš„æŸå¤±å‡½æ•°å€¼å˜åŒ–", fontsize=14, fontweight='bold')
plt.xlabel("è¿­ä»£æ¬¡æ•°ï¼ˆæ¯æ¬¡å¤„ç† 100 å¼ å›¾ç‰‡ï¼‰", fontsize=12)
plt.ylabel("æŸå¤±å‡½æ•°å€¼ï¼ˆäº¤å‰ç†µï¼‰", fontsize=12)

# æ·»åŠ ç½‘æ ¼çº¿ï¼Œä¾¿äºè§‚å¯Ÿ
plt.grid(True, alpha=0.3)

# æ·»åŠ æ–‡æœ¬è¯´æ˜
final_loss = train_loss_list[-1]
plt.text(len(train_loss_list) * 0.7, max(train_loss_list) * 0.9, 
         f'æœ€ç»ˆæŸå¤±å€¼: {final_loss:.4f}', 
         fontsize=11, bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))

# ç´§å¯†å¸ƒå±€ï¼ˆé˜²æ­¢æ ‡ç­¾è¢«åˆ‡å‰²ï¼‰
plt.tight_layout()

# æ˜¾ç¤ºå›¾è¡¨
plt.show()

# æ‰“å°è®­ç»ƒç»Ÿè®¡ä¿¡æ¯
print(f"\n========== è®­ç»ƒå®Œæˆ ==========")
print(f"æ€»è¿­ä»£æ¬¡æ•°: {iters_num}")
print(f"åˆå§‹æŸå¤±å€¼: {train_loss_list[0]:.4f}")
print(f"æœ€ç»ˆæŸå¤±å€¼: {train_loss_list[-1]:.4f}")
print(f"æŸå¤±ä¸‹é™å¹…åº¦: {(train_loss_list[0] - train_loss_list[-1]):.4f}")

```

é»˜è®¤æ˜¯ä½¿ç”¨ **CPU** è¿›è¡Œè®­ç»ƒçš„

* NumPy åªåœ¨ CPU ä¸Šè¿è¡Œ
* ä¸æ”¯æŒ GPU åŠ é€Ÿ
* æ‰€æœ‰çŸ©é˜µè¿ç®—ï¼ˆå‰å‘ã€æ¢¯åº¦ã€æ›´æ–°ï¼‰éƒ½æ˜¯ CPU å®Œæˆ

**æ€»ç»“**

| é—®é¢˜           | ç­”æ¡ˆ                          |
| ------------ | --------------------------- |
| è¿™æ®µä»£ç é»˜è®¤GPUè®­ç»ƒå— | âŒ ä¸æ˜¯                        |
| ç°åœ¨è·‘åœ¨å“ª        | CPU                         |
| ä¸ºä»€ä¹ˆ          | ç”¨çš„æ˜¯ NumPy                   |
| æƒ³ç”¨GPUæ€ä¹ˆåŠ     | PyTorch / TensorFlow / CuPy |
| æœ€å¤§æ€§èƒ½ç“¶é¢ˆåœ¨å“ª     | numerical_gradientï¼ˆæ•°å€¼æ±‚å¯¼ï¼‰    |

**è®­ç»ƒæ—¶é—´å¯¹æ¯”**

AMD Ryzen 9 5950X 16-Core Processor

| æ–¹å¼                        | åŸç†           | è®­ç»ƒæ—¶é—´        |
| ------------------------- | ------------ | ----------- |
| numerical_gradientï¼ˆä½ ç°åœ¨ç”¨çš„ï¼‰ | æ•°å€¼å¾®åˆ†ï¼ˆé€å‚æ•°æ‰°åŠ¨ï¼‰  | **8ï½30 å°æ—¶** |
| åå‘ä¼ æ’­ gradient             | é“¾å¼æ³•åˆ™ä¸€æ¬¡ç®—å®Œæ‰€æœ‰æ¢¯åº¦ | **1ï½3 åˆ†é’Ÿ**  |

### æ”¹æˆåå‘ä¼ æ’­è®­ç»ƒ

```py

    # è®¡ç®—æ¢¯åº¦ï¼šæ¢¯åº¦å‘Šè¯‰æˆ‘ä»¬å‚æ•°åº”è¯¥å¾€å“ªä¸ªæ–¹å‘è°ƒæ•´æ‰èƒ½å‡å°æŸå¤±
    # grad = network.numerical_gradient(x_batch, t_batch)
    grad = network.gradient(x_batch, t_batch)  # å¯ä»¥ç”¨æ›´å¿«çš„åå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦

```

**æ¨¡å‹æ€§èƒ½**

* åˆå§‹æŸå¤±å€¼ï¼š2.2864ï¼ˆæœªè®­ç»ƒçŠ¶æ€ï¼‰
* æœ€ç»ˆæŸå¤±å€¼ï¼š0.0972ï¼ˆè®­ç»ƒåï¼‰
* æŸå¤±ä¸‹é™å¹…åº¦ï¼š95.7%ï¼ˆä» 2.2864 é™è‡³ 0.0972ï¼‰

**è®­ç»ƒæ•ˆç‡**

* æ€»è¿­ä»£æ¬¡æ•°ï¼š10,000 æ­¥
* æ€»è®­ç»ƒè€—æ—¶ï¼š15.75 ç§’
* å¹³å‡æ¯æ­¥è€—æ—¶ï¼š1.57 æ¯«ç§’

**æ•°æ®é›†ä¸ç½‘ç»œé…ç½®**

* MNIST æ‰‹å†™æ•°å­—æ•°æ®é›†ï¼ˆ60,000 å¼ è®­ç»ƒå›¾ç‰‡ï¼‰
* ç½‘ç»œç»“æ„ï¼š784 â†’ 100 â†’ 10ï¼ˆä¸¤å±‚ç¥ç»ç½‘ç»œï¼‰
* æ‰¹å¤„ç†å¤§å°ï¼šæ¯æ‰¹ 100 å¼ å›¾ç‰‡
* å­¦ä¹ ç‡ï¼š0.1

**é¢„æµ‹å‡†ç¡®ç‡**

æµ‹è¯•é›†å‡†ç¡®ç‡ï¼š92.07%

![mnist_train](/images/ai/mnist_train.png)




























