---
layout:     post
title:      æ·±åº¦å­¦ä¹ äºŒ
subtitle:   åŸºäºPythonçš„ç†è®ºå’Œå®ç°
date:       2026-02-12
author:     LXG
header-img: img/post-bg-universe.jpg
catalog: true
tags:
    - AI
---

[2026 Agent ç¼–ç¨‹è¶‹åŠ¿æŠ¥å‘Š-Anthropic](https://baoyu.io/blog/2026/02/09/anthropic-agentic-coding-trends-2026)

## ä¸¤å±‚ç¥ç»ç½‘ç»œ 

```py

import sys, os

import numpy as np

# sigmoid å‡½æ•°ï¼šæŠŠä»»æ„æ•°å€¼å‹ç¼©åˆ° 0-1 ä¹‹é—´ï¼ˆç”¨äºéšè—å±‚ï¼‰
def sigmoid(x):
    return 1 / (1 + np.exp(-x))    


def sigmoid_grad(x):
    """sigmoid å‡½æ•°çš„å¯¼æ•°ï¼šç”¨äºåå‘ä¼ æ’­æ—¶è®¡ç®—æ¢¯åº¦
    
    æ•°å­¦å½¢å¼ï¼šd(sigmoid)/dx = sigmoid(x) * (1 - sigmoid(x))
    """
    return (1.0 - sigmoid(x)) * sigmoid(x)

# softmax å‡½æ•°ï¼šæŠŠä»»æ„æ•°å€¼è½¬æ¢æˆ 0-1 ä¹‹é—´çš„æ¦‚ç‡ï¼ˆç”¨äºè¾“å‡ºå±‚ï¼‰
def softmax(x):
    if x.ndim == 2:
        x = x.T
        x = x - np.max(x, axis=0)
        y = np.exp(x) / np.sum(np.exp(x), axis=0)
        return y.T 

    x = x - np.max(x) # å‡å»æœ€å¤§å€¼ï¼Œé˜²æ­¢æ•°å€¼æº¢å‡ºï¼ˆè®¡ç®—ç¨³å®šæ€§æŠ€å·§ï¼‰
    return np.exp(x) / np.sum(np.exp(x)) # æŒ‡æ•°å½’ä¸€åŒ–æˆæ¦‚ç‡

def cross_entropy_error(y, t):
    """äº¤å‰ç†µæŸå¤±å‡½æ•°ï¼šè¡¡é‡æ¨¡å‹é¢„æµ‹å’ŒçœŸå®æ ‡ç­¾çš„å·®è·
    
    å‚æ•°
    -----
    y : numpy.ndarray
        æ¨¡å‹é¢„æµ‹çš„æ¦‚ç‡åˆ†å¸ƒï¼ˆç”± softmax è¾“å‡ºï¼‰
    t : numpy.ndarray
        çœŸå®æ ‡ç­¾ï¼ˆç‹¬çƒ­ç¼–ç æˆ–ç±»åˆ«ç´¢å¼•ï¼‰
    
    è¿”å›
    ----
    float
        äº¤å‰ç†µæŸå¤±ï¼ˆæ•°å€¼è¶Šå°è¡¨ç¤ºé¢„æµ‹è¶Šå‡†ç¡®ï¼‰
    """
    if y.ndim == 1:
        t = t.reshape(1, t.size)
        y = y.reshape(1, y.size)
        
    # å¦‚æœæ ‡ç­¾æ˜¯ç‹¬çƒ­ç¼–ç ï¼ˆå¦‚ [0, 1, 0]ï¼‰ï¼Œè½¬æ¢æˆç±»åˆ«ç´¢å¼•ï¼ˆå¦‚ 1ï¼‰
    if t.size == y.size:
        t = t.argmax(axis=1)
             
    batch_size = y.shape[0]
    # è®¡ç®—å¹³å‡çš„äº¤å‰ç†µï¼ˆåŠ ä¸ªå¾ˆå°çš„å€¼ 1e-7 é˜²æ­¢å–å¯¹æ•°æ—¶å‡ºç°è´Ÿæ— ç©·ï¼‰
    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size

def numerical_gradient(f, x):
    """æ•°å€¼æ¢¯åº¦è®¡ç®—å‡½æ•°ï¼šç”¨æœ‰é™å·®åˆ†æ³•è¿‘ä¼¼è®¡ç®—å‡½æ•° f å¯¹ x çš„æ¢¯åº¦
    
    åŸç†ï¼šæ¢¯åº¦ â‰ˆ (f(x+h) - f(x-h)) / (2*h)ï¼Œå…¶ä¸­ h æ˜¯ä¸€ä¸ªå¾ˆå°çš„æ•°
    
    å‚æ•°
    -----
    f : callable
        ç›®æ ‡å‡½æ•°ï¼ˆæ¥å— x è¿”å›æ ‡é‡ï¼‰
    x : numpy.ndarray
        è¾“å…¥å˜é‡ï¼ˆå¯ä»¥æ˜¯å‘é‡æˆ–çŸ©é˜µï¼‰
    
    è¿”å›
    ----
    grad : numpy.ndarray
        æ¢¯åº¦ï¼Œä¸ x å½¢çŠ¶ç›¸åŒ
    """
    h = 1e-4 # å¾ˆå°çš„æ­¥é•¿ï¼ˆ0.0001ï¼‰ï¼Œç”¨æ¥è¿‘ä¼¼å¯¼æ•°
    grad = np.zeros_like(x)  # åˆå§‹åŒ–æ¢¯åº¦æ•°ç»„
    
    # éå† x çš„æ¯ä¸ªå…ƒç´ 
    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])
    while not it.finished:
        idx = it.multi_index  # è·å–å½“å‰å…ƒç´ çš„ç´¢å¼•
        tmp_val = x[idx]  # ä¿å­˜åŸå§‹å€¼
        
        # è®¡ç®— f(x+h)ï¼šç¬¬ idx ä¸ªå…ƒç´ å¢åŠ  h
        x[idx] = float(tmp_val) + h
        fxh1 = f(x)
        
        # è®¡ç®— f(x-h)ï¼šç¬¬ idx ä¸ªå…ƒç´ å‡å°‘ h
        x[idx] = tmp_val - h 
        fxh2 = f(x)
        
        # ç”¨ä¸­å¿ƒå·®åˆ†å…¬å¼è®¡ç®—æ¢¯åº¦ï¼ˆåå¯¼æ•°ï¼‰
        grad[idx] = (fxh1 - fxh2) / (2*h)
        
        # æ¢å¤åŸå§‹å€¼ï¼Œå‡†å¤‡è®¡ç®—ä¸‹ä¸€ä¸ªå…ƒç´ 
        x[idx] = tmp_val
        it.iternext()   
        
    return grad

class TwoLayerNet:
    """ä¸¤å±‚ç¥ç»ç½‘ç»œç±»ï¼šè¾“å…¥ -> éšè—å±‚ -> è¾“å‡ºå±‚
    
    è¿™æ˜¯ä¸€ä¸ªç®€å•çš„å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰ï¼Œç”¨ sigmoid æ¿€æ´»éšè—å±‚ï¼Œsoftmax æ¿€æ´»è¾“å‡ºå±‚ã€‚
    """
    
    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):
        """åˆå§‹åŒ–ç½‘ç»œçš„æƒé‡å’Œåç½®ï¼ˆç”¨éšæœºæ•°å¡«å……ï¼‰ã€‚
        
        å‚æ•°
        -----
        input_size : è¾“å…¥èŠ‚ç‚¹ä¸ªæ•°
        hidden_size : éšè—å±‚èŠ‚ç‚¹ä¸ªæ•°
        output_size : è¾“å‡ºèŠ‚ç‚¹ä¸ªæ•°
        weight_init_std : æƒé‡åˆå§‹åŒ–çš„éšæœºæ•°å¤§å°ï¼ˆè¾ƒå°çš„æ•°æœ‰åˆ©äºè®­ç»ƒï¼‰
        """
        # åˆ›å»ºç½‘ç»œå‚æ•°å­—å…¸ï¼Œå­˜æ”¾æ‰€æœ‰æƒé‡å’Œåç½®
        self.params = {}
        # ç¬¬ä¸€å±‚ï¼ˆè¾“å…¥åˆ°éšè—å±‚ï¼‰çš„æƒé‡å’Œåç½®
        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)
        self.params['b1'] = np.zeros(hidden_size)
        # ç¬¬äºŒå±‚ï¼ˆéšè—å±‚åˆ°è¾“å‡ºå±‚ï¼‰çš„æƒé‡å’Œåç½®
        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)
        self.params['b2'] = np.zeros(output_size)

    def predict(self, x):
        """å‰å‘ä¼ æ’­ï¼šè¾“å…¥æ•°æ®é€šè¿‡ç½‘ç»œå¾—åˆ°é¢„æµ‹ç»“æœã€‚
        
        è®¡ç®—è¿‡ç¨‹ï¼šè¾“å…¥ -> (W1, b1) -> sigmoid -> (W2, b2) -> softmax -> è¾“å‡ºæ¦‚ç‡
        """
        W1, W2 = self.params['W1'], self.params['W2']
        b1, b2 = self.params['b1'], self.params['b2']
    
        # ç¬¬ä¸€å±‚ï¼šè¾“å…¥ dot æƒé‡ + åç½®ï¼Œç„¶åé€šè¿‡ sigmoid æ¿€æ´»
        a1 = np.dot(x, W1) + b1
        z1 = sigmoid(a1)
        
        # ç¬¬äºŒå±‚ï¼šéšè—å±‚è¾“å‡º dot æƒé‡ + åç½®ï¼Œç„¶åé€šè¿‡ softmax æ¿€æ´»å¾—åˆ°æ¦‚ç‡
        a2 = np.dot(z1, W2) + b2
        y = softmax(a2)
        
        return y

    def loss(self, x, t):
        """è®¡ç®—æŸå¤±å‡½æ•°å€¼ï¼šè¡¡é‡ç½‘ç»œé¢„æµ‹ä¸çœŸå®æ ‡ç­¾çš„è¯¯å·®
        
        å‚æ•°
        -----
        x : numpy.ndarray
            è¾“å…¥æ•°æ®ï¼Œå½¢çŠ¶ (æ ·æœ¬æ•°, è¾“å…¥ç»´åº¦)
        t : numpy.ndarray
            çœŸå®æ ‡ç­¾ï¼Œå½¢çŠ¶ (æ ·æœ¬æ•°, è¾“å‡ºç»´åº¦) æˆ– (æ ·æœ¬æ•°,)
        
        è¿”å›
        ----
        float
            äº¤å‰ç†µæŸå¤±å€¼
        """
        y = self.predict(x)  # å‰å‘ä¼ æ’­å¾—åˆ°é¢„æµ‹ç»“æœ
        
        return cross_entropy_error(y, t)  # è®¡ç®—æŸå¤±

    def accuracy(self, x, t):
        """è®¡ç®—ç½‘ç»œçš„å‡†ç¡®ç‡ï¼šé¢„æµ‹æ­£ç¡®çš„æ ·æœ¬æ•°é‡å æ¯”
        
        å‚æ•°
        -----
        x : numpy.ndarray
            è¾“å…¥æ•°æ®ï¼Œå½¢çŠ¶ (æ ·æœ¬æ•°, è¾“å…¥ç»´åº¦)
        t : numpy.ndarray
            çœŸå®æ ‡ç­¾ï¼Œå½¢çŠ¶ (æ ·æœ¬æ•°, è¾“å‡ºç»´åº¦)ï¼ˆé€šå¸¸æ˜¯ç‹¬çƒ­ç¼–ç ï¼‰
        
        è¿”å›
        ----
        float
            å‡†ç¡®ç‡ï¼ŒèŒƒå›´ 0-1ï¼ˆ0 = å…¨é”™ï¼Œ1 = å…¨å¯¹ï¼‰
        """
        y = self.predict(x)  # å‰å‘ä¼ æ’­å¾—åˆ°é¢„æµ‹æ¦‚ç‡
        y = np.argmax(y, axis=1)  # é€‰æ‹©æ¦‚ç‡æœ€å¤§çš„ç±»åˆ«ä½œä¸ºé¢„æµ‹ç»“æœ
        t = np.argmax(t, axis=1)  # ä»ç‹¬çƒ­ç¼–ç è½¬æ¢ä¸ºç±»åˆ«ç´¢å¼•
        
        # æ¯”è¾ƒé¢„æµ‹å’ŒçœŸå®æ ‡ç­¾ï¼Œè®¡ç®—ç›¸ç­‰çš„æ•°é‡å æ¯”
        accuracy = np.sum(y == t) / float(x.shape[0])
        return accuracy
            
    def numerical_gradient(self, x, t):
        """ç”¨æ•°å€¼å¾®åˆ†æ³•è®¡ç®—æ‰€æœ‰å‚æ•°çš„æ¢¯åº¦ï¼ˆé€Ÿåº¦æ…¢ï¼Œä½†æ˜“äºç†è§£å’ŒéªŒè¯ï¼‰
        
        å‚æ•°
        -----
        x : numpy.ndarray
            è¾“å…¥æ•°æ®ï¼Œå½¢çŠ¶ (æ ·æœ¬æ•°, è¾“å…¥ç»´åº¦)
        t : numpy.ndarray
            çœŸå®æ ‡ç­¾ï¼Œå½¢çŠ¶ (æ ·æœ¬æ•°, è¾“å‡ºç»´åº¦)
        
        è¿”å›
        ----
        grads : dict
            æ¢¯åº¦å­—å…¸ï¼ŒåŒ…å« 'W1', 'b1', 'W2', 'b2' å››ä¸ªæ¢¯åº¦å€¼
        """
        # å®šä¹‰ä¸€ä¸ªå…³äºæƒé‡ W çš„æŸå¤±å‡½æ•°ï¼ˆç”¨äºæ•°å€¼å¾®åˆ†ï¼‰
        loss_W = lambda W: self.loss(x, t)
        
        # è®¡ç®—æ¯ä¸ªå‚æ•°çš„æ¢¯åº¦
        grads = {}
        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])
        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])
        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])
        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])
        
        return grads
        
    def gradient(self, x, t):
        """ç”¨åå‘ä¼ æ’­ç®—æ³•è®¡ç®—æ‰€æœ‰å‚æ•°çš„æ¢¯åº¦ï¼ˆæ¯”æ•°å€¼å¾®åˆ†å¿«å¾—å¤šï¼‰
        
        è¿™æ˜¯æ·±åº¦å­¦ä¹ ä¸­æœ€å¸¸ç”¨çš„æ¢¯åº¦è®¡ç®—æ–¹æ³•ï¼Œé€šè¿‡é“¾å¼æ³•åˆ™ä»è¾“å‡ºå±‚åå‘è®¡ç®—æ¯å±‚çš„æ¢¯åº¦ã€‚
        
        å‚æ•°
        -----
        x : numpy.ndarray
            è¾“å…¥æ•°æ®ï¼Œå½¢çŠ¶ (æ ·æœ¬æ•°, è¾“å…¥ç»´åº¦)
        t : numpy.ndarray
            çœŸå®æ ‡ç­¾ï¼Œå½¢çŠ¶ (æ ·æœ¬æ•°, è¾“å‡ºç»´åº¦)
        
        è¿”å›
        ----
        grads : dict
            æ¢¯åº¦å­—å…¸ï¼ŒåŒ…å« 'W1', 'b1', 'W2', 'b2' å››ä¸ªæ¢¯åº¦å€¼
        """
        W1, W2 = self.params['W1'], self.params['W2']
        b1, b2 = self.params['b1'], self.params['b2']
        grads = {}
        
        batch_num = x.shape[0]  # æ‰¹æ¬¡ä¸­æ ·æœ¬çš„ä¸ªæ•°
        
        # ========== å‰å‘ä¼ æ’­ï¼šè®¡ç®—ç½‘ç»œè¾“å‡º ==========
        a1 = np.dot(x, W1) + b1  # ç¬¬ä¸€å±‚çš„çº¿æ€§å˜æ¢
        z1 = sigmoid(a1)  # ç¬¬ä¸€å±‚çš„æ¿€æ´»å‡½æ•°
        a2 = np.dot(z1, W2) + b2  # ç¬¬äºŒå±‚çš„çº¿æ€§å˜æ¢
        y = softmax(a2)  # ç¬¬äºŒå±‚çš„æ¿€æ´»å‡½æ•°ï¼ˆè¾“å‡ºæ¦‚ç‡ï¼‰
        
        # ========== åå‘ä¼ æ’­ï¼šä»è¾“å‡ºå±‚å¼€å§‹åå‘è®¡ç®—æ¢¯åº¦ ==========
        # ç¬¬äºŒå±‚çš„æ¢¯åº¦ï¼š(é¢„æµ‹æ¦‚ç‡ - çœŸå®æ ‡ç­¾) / æ ·æœ¬æ•°ï¼ˆè¿™æ¥è‡ªäº¤å‰ç†µ+ softmax çš„æ±‚å¯¼ï¼‰
        dy = (y - t) / batch_num
        # è®¡ç®—ç¬¬äºŒå±‚å‚æ•°çš„æ¢¯åº¦
        grads['W2'] = np.dot(z1.T, dy)  # éšè—å±‚æ¿€æ´»çš„è½¬ç½® dot è¾“å‡ºè¯¯å·®
        grads['b2'] = np.sum(dy, axis=0)  # æ²¿æ ·æœ¬ç»´åº¦æ±‚å’Œå¾—åˆ°åç½®æ¢¯åº¦
        
        # ç¬¬ä¸€å±‚çš„æ¢¯åº¦ï¼šåå‘ä¼ æ’­è¯¯å·®
        da1 = np.dot(dy, W2.T)  # è¯¯å·®é€šè¿‡æƒé‡ W2 åå‘ä¼ æ’­
        dz1 = sigmoid_grad(a1) * da1  # ä¹˜ä»¥ sigmoid å¯¼æ•°ï¼ˆé“¾å¼æ³•åˆ™ï¼‰
        # è®¡ç®—ç¬¬ä¸€å±‚å‚æ•°çš„æ¢¯åº¦
        grads['W1'] = np.dot(x.T, dz1)  # è¾“å…¥çš„è½¬ç½® dot ç¬¬ä¸€å±‚è¯¯å·®
        grads['b1'] = np.sum(dz1, axis=0)  # æ²¿æ ·æœ¬ç»´åº¦æ±‚å’Œå¾—åˆ°åç½®æ¢¯åº¦

        return grads

```

### å‡½æ•°è§£é‡Š  

| å‡½æ•°å | åŠŸèƒ½æè¿° | ç”¨é€” |
|--------|--------|------|
| `sigmoid(x)` | å°†ä»»æ„æ•°å€¼å‹ç¼©åˆ° 0-1 ä¹‹é—´ | éšè—å±‚æ¿€æ´»å‡½æ•° |
| `sigmoid_grad(x)` | è®¡ç®— sigmoid çš„å¯¼æ•°å€¼ | åå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦ |
| `softmax(x)` | å°†ä»»æ„æ•°å€¼è½¬æ¢ä¸º 0-1 çš„æ¦‚ç‡åˆ†å¸ƒ | è¾“å‡ºå±‚æ¿€æ´»å‡½æ•° |
| `cross_entropy_error(y, t)` | è®¡ç®—æ¨¡å‹é¢„æµ‹ä¸çœŸå®æ ‡ç­¾çš„è¯¯å·® | æŸå¤±å‡½æ•° |
| `numerical_gradient(f, x)` | ä½¿ç”¨æœ‰é™å·®åˆ†æ³•è®¡ç®—å‡½æ•°æ¢¯åº¦ | éªŒè¯åå‘ä¼ æ’­ç®—æ³•æ­£ç¡®æ€§ |
| `TwoLayerNet.__init__()` | åˆå§‹åŒ–ç½‘ç»œçš„æƒé‡å’Œåç½® | ç½‘ç»œå‚æ•°åˆå§‹åŒ– |
| `TwoLayerNet.predict(x)` | å‰å‘ä¼ æ’­è®¡ç®—ç½‘ç»œé¢„æµ‹å€¼ | é¢„æµ‹è¾“å‡º |
| `TwoLayerNet.loss(x, t)` | è®¡ç®—äº¤å‰ç†µæŸå¤±å‡½æ•°å€¼ | è¯„ä¼°è®­ç»ƒæ•ˆæœ |
| `TwoLayerNet.accuracy(x, t)` | è®¡ç®—åˆ†ç±»å‡†ç¡®ç‡ | è¯„ä¼°æ¨¡å‹æ€§èƒ½ |
| `TwoLayerNet.numerical_gradient(x, t)` | ç”¨æ•°å€¼å¾®åˆ†æ³•è®¡ç®—æ¢¯åº¦ | æ¢¯åº¦éªŒè¯ç”¨ |
| `TwoLayerNet.gradient(x, t)` | ç”¨åå‘ä¼ æ’­ç®—æ³•è®¡ç®—æ¢¯åº¦ | è®­ç»ƒç½‘ç»œç”¨ï¼ˆæ¨èï¼‰ |

### è®­ç»ƒæµç¨‹

å‰å‘ä¼ æ’­ â†’ è®¡ç®—æŸå¤± â†’ åå‘ä¼ æ’­ â†’ æ›´æ–°å‚æ•° â†’ å¾ªç¯

### æ¢¯åº¦è®¡ç®—æ–¹æ³•å¯¹æ¯”

| æ¢¯åº¦è®¡ç®—æ–¹æ³• | è®¡ç®—é€Ÿåº¦ | æ˜“ç”¨æ€§ | åº”ç”¨åœºæ™¯ |
|-------------|--------|-------|--------|
| æ•°å€¼å¾®åˆ† | ğŸ”´ å¾ˆæ…¢ | ğŸŸ¢ æ˜“äºç†è§£ | éªŒè¯å’Œå­¦ä¹ ç”¨ |
| åå‘ä¼ æ’­ | ğŸŸ¢ å¾ˆå¿« | ğŸŸ¡ éœ€è¦æ•°å­¦åŸºç¡€ | å®é™…è®­ç»ƒç”¨ |

### MNIST æ•°æ®è®­ç»ƒ

```py

import numpy as np
from mnist_demo import load_mnist
from two_layer import TwoLayerNet

# åŠ è½½ MNIST æ‰‹å†™æ•°å­—æ•°æ®é›†
# normalize=Trueï¼šåƒç´ å€¼ç¼©æ”¾åˆ° 0-1ï¼ˆä¾¿äºç¥ç»ç½‘ç»œå¤„ç†ï¼‰
# flatten=Trueï¼šæŠŠ 28x28 çš„å›¾ç‰‡å±•å¹³æˆ 784 ç»´å‘é‡
# one_hot_label=Trueï¼šæ ‡ç­¾è½¬æ¢æˆç‹¬çƒ­ç¼–ç ï¼ˆä¾‹å¦‚ 5 å˜æˆ [0,0,0,0,0,1,0,0,0,0]ï¼‰
(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, flatten=True, one_hot_label=True)

train_loss_list = []  # å­˜å‚¨æ¯æ¬¡è¿­ä»£çš„æŸå¤±å€¼ï¼Œç”¨äºç”»å›¾è§‚å¯Ÿè®­ç»ƒè¿‡ç¨‹

# ========== è®­ç»ƒè¶…å‚æ•°è®¾ç½® ==========
# è¿™äº›æ˜¯ç¥ç»ç½‘ç»œè®­ç»ƒæ—¶éœ€è¦æ‰‹åŠ¨è°ƒæ•´çš„å‚æ•°
iters_num = 10000  # æ€»å…±è®­ç»ƒå¤šå°‘æ­¥ï¼ˆæ¯æ­¥å¤„ç†ä¸€ä¸ªå°æ‰¹é‡æ•°æ®ï¼‰
train_size = x_train.shape[0]  # è®­ç»ƒé›†ä¸­æœ‰å¤šå°‘å¼ å›¾ç‰‡
batch_size = 100  # æ¯æ¬¡è®­ç»ƒç”¨å¤šå°‘å¼ å›¾ç‰‡ï¼ˆ100 å¼ ä¸ºä¸€ä¸ªæ‰¹æ¬¡ï¼‰
learning_rate = 0.1  # å­¦ä¹ ç‡ï¼šæ§åˆ¶æ¯æ¬¡å‚æ•°æ›´æ–°çš„å¹…åº¦ï¼ˆæ•°å€¼è¶Šå¤§å­¦ä¹ è¶Šå¿«ï¼Œä½†å¯èƒ½è·³è¿‡æœ€ä¼˜å€¼ï¼‰

# åˆå§‹åŒ–ç¥ç»ç½‘ç»œï¼š784 ä¸ªè¾“å…¥ -> 100 ä¸ªéšè—å±‚èŠ‚ç‚¹ -> 10 ä¸ªè¾“å‡ºï¼ˆ0-9 åä¸ªæ•°å­—ï¼‰
network = TwoLayerNet(input_size=784, hidden_size=100, output_size=10)

# ========== å¼€å§‹è®­ç»ƒå¾ªç¯ ==========
for i in range(iters_num):
    # ä»è®­ç»ƒé›†ä¸­éšæœºæŠ½å– batch_size å¼ å›¾ç‰‡ï¼ˆå°æ‰¹é‡éšæœºæŠ½æ ·ï¼Œè¿™æ ·è®­ç»ƒæ›´ç¨³å®šï¼‰
    batch_mask = np.random.choice(train_size, batch_size)
    x_batch = x_train[batch_mask]  # æŠ½å–çš„å›¾ç‰‡æ•°æ®
    t_batch = t_train[batch_mask]  # æŠ½å–çš„å›¾ç‰‡æ ‡ç­¾

    # è®¡ç®—æ¢¯åº¦ï¼šæ¢¯åº¦å‘Šè¯‰æˆ‘ä»¬å‚æ•°åº”è¯¥å¾€å“ªä¸ªæ–¹å‘è°ƒæ•´æ‰èƒ½å‡å°æŸå¤±
    grad = network.numerical_gradient(x_batch, t_batch)
    # grad = network.gradient(x_batch, t_batch)  # å¯ä»¥ç”¨æ›´å¿«çš„åå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦

    # å‚æ•°æ›´æ–°ï¼šæ²¿ç€æ¢¯åº¦çš„åæ–¹å‘ï¼ˆä¸‹é™æ–¹å‘ï¼‰è°ƒæ•´å‚æ•°
    # æ–°å‚æ•° = æ—§å‚æ•° - å­¦ä¹ ç‡ Ã— æ¢¯åº¦
    for key in ('W1', 'b1', 'W2', 'b2'):
        network.params[key] -= learning_rate * grad[key]

    # è®¡ç®—å½“å‰å°æ‰¹é‡çš„æŸå¤±å€¼ï¼ˆè¡¡é‡é¢„æµ‹æœ‰å¤šé”™è¯¯ï¼‰
    loss = network.loss(x_batch, t_batch)
    train_loss_list.append(loss)  # è®°å½•è¿™ä¸€æ­¥çš„æŸå¤±å€¼

    # æ¯ 1000 æ­¥æ‰“å°ä¸€æ¬¡è¿›åº¦å’ŒæŸå¤±å€¼
    if i % 1000 == 0:
        print(f"è®­ç»ƒè¿›åº¦: ç¬¬ {i} æ­¥ï¼Œå½“å‰æŸå¤±å€¼: {loss:.4f}")


# ========== è®­ç»ƒå®Œæˆï¼Œç»˜åˆ¶æŸå¤±å‡½æ•°æ›²çº¿ ==========
import matplotlib.pyplot as plt

# åˆ›å»ºå›¾è¡¨
plt.figure(figsize=(12, 6))

# ç»˜åˆ¶æŸå¤±å‡½æ•°å€¼æ¨ç§»æ›²çº¿
plt.plot(train_loss_list, linewidth=0.5)

# è®¾ç½®å›¾è¡¨æ ‡é¢˜å’Œæ ‡ç­¾
plt.title("è®­ç»ƒè¿‡ç¨‹ä¸­çš„æŸå¤±å‡½æ•°å€¼å˜åŒ–", fontsize=14, fontweight='bold')
plt.xlabel("è¿­ä»£æ¬¡æ•°ï¼ˆæ¯æ¬¡å¤„ç† 100 å¼ å›¾ç‰‡ï¼‰", fontsize=12)
plt.ylabel("æŸå¤±å‡½æ•°å€¼ï¼ˆäº¤å‰ç†µï¼‰", fontsize=12)

# æ·»åŠ ç½‘æ ¼çº¿ï¼Œä¾¿äºè§‚å¯Ÿ
plt.grid(True, alpha=0.3)

# æ·»åŠ æ–‡æœ¬è¯´æ˜
final_loss = train_loss_list[-1]
plt.text(len(train_loss_list) * 0.7, max(train_loss_list) * 0.9, 
         f'æœ€ç»ˆæŸå¤±å€¼: {final_loss:.4f}', 
         fontsize=11, bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))

# ç´§å¯†å¸ƒå±€ï¼ˆé˜²æ­¢æ ‡ç­¾è¢«åˆ‡å‰²ï¼‰
plt.tight_layout()

# æ˜¾ç¤ºå›¾è¡¨
plt.show()

# æ‰“å°è®­ç»ƒç»Ÿè®¡ä¿¡æ¯
print(f"\n========== è®­ç»ƒå®Œæˆ ==========")
print(f"æ€»è¿­ä»£æ¬¡æ•°: {iters_num}")
print(f"åˆå§‹æŸå¤±å€¼: {train_loss_list[0]:.4f}")
print(f"æœ€ç»ˆæŸå¤±å€¼: {train_loss_list[-1]:.4f}")
print(f"æŸå¤±ä¸‹é™å¹…åº¦: {(train_loss_list[0] - train_loss_list[-1]):.4f}")

```

é»˜è®¤æ˜¯ä½¿ç”¨ **CPU** è¿›è¡Œè®­ç»ƒçš„

* NumPy åªåœ¨ CPU ä¸Šè¿è¡Œ
* ä¸æ”¯æŒ GPU åŠ é€Ÿ
* æ‰€æœ‰çŸ©é˜µè¿ç®—ï¼ˆå‰å‘ã€æ¢¯åº¦ã€æ›´æ–°ï¼‰éƒ½æ˜¯ CPU å®Œæˆ

**æ€»ç»“**

| é—®é¢˜           | ç­”æ¡ˆ                          |
| ------------ | --------------------------- |
| è¿™æ®µä»£ç é»˜è®¤GPUè®­ç»ƒå— | âŒ ä¸æ˜¯                        |
| ç°åœ¨è·‘åœ¨å“ª        | CPU                         |
| ä¸ºä»€ä¹ˆ          | ç”¨çš„æ˜¯ NumPy                   |
| æƒ³ç”¨GPUæ€ä¹ˆåŠ     | PyTorch / TensorFlow / CuPy |
| æœ€å¤§æ€§èƒ½ç“¶é¢ˆåœ¨å“ª     | numerical_gradientï¼ˆæ•°å€¼æ±‚å¯¼ï¼‰    |

**è®­ç»ƒæ—¶é—´å¯¹æ¯”**

AMD Ryzen 9 5950X 16-Core Processor

| æ–¹å¼                        | åŸç†           | è®­ç»ƒæ—¶é—´        |
| ------------------------- | ------------ | ----------- |
| numerical_gradientï¼ˆä½ ç°åœ¨ç”¨çš„ï¼‰ | æ•°å€¼å¾®åˆ†ï¼ˆé€å‚æ•°æ‰°åŠ¨ï¼‰  | **8ï½30 å°æ—¶** |
| åå‘ä¼ æ’­ gradient             | é“¾å¼æ³•åˆ™ä¸€æ¬¡ç®—å®Œæ‰€æœ‰æ¢¯åº¦ | **1ï½3 åˆ†é’Ÿ**  |

### æ”¹æˆåå‘ä¼ æ’­è®­ç»ƒ

```py

    # è®¡ç®—æ¢¯åº¦ï¼šæ¢¯åº¦å‘Šè¯‰æˆ‘ä»¬å‚æ•°åº”è¯¥å¾€å“ªä¸ªæ–¹å‘è°ƒæ•´æ‰èƒ½å‡å°æŸå¤±
    # grad = network.numerical_gradient(x_batch, t_batch)
    grad = network.gradient(x_batch, t_batch)  # å¯ä»¥ç”¨æ›´å¿«çš„åå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦

```

**è®­ç»ƒè¿‡ç¨‹**

```bash

ç¬¬ 0 æ­¥ï¼š2.2942
ç¬¬ 1000 æ­¥ï¼š0.5611
ç¬¬ 2000 æ­¥ï¼š0.4726
ç¬¬ 3000 æ­¥ï¼š0.3393
ç¬¬ 4000 æ­¥ï¼š0.2660
ç¬¬ 5000 æ­¥ï¼š0.2507
ç¬¬ 6000 æ­¥ï¼š0.2323
ç¬¬ 7000 æ­¥ï¼š0.3340
ç¬¬ 8000 æ­¥ï¼š0.2121
ç¬¬ 9000 æ­¥ï¼š0.1369
æœ€ç»ˆï¼š0.2750

```

**æ¨¡å‹æ€§èƒ½**

* åˆå§‹æŸå¤±å€¼ï¼š2.2864ï¼ˆæœªè®­ç»ƒçŠ¶æ€ï¼‰
* æœ€ç»ˆæŸå¤±å€¼ï¼š0.0972ï¼ˆè®­ç»ƒåï¼‰
* æŸå¤±ä¸‹é™å¹…åº¦ï¼š95.7%ï¼ˆä» 2.2864 é™è‡³ 0.0972ï¼‰

**è®­ç»ƒæ•ˆç‡**

* æ€»è¿­ä»£æ¬¡æ•°ï¼š10,000 æ­¥
* æ€»è®­ç»ƒè€—æ—¶ï¼š15.75 ç§’
* å¹³å‡æ¯æ­¥è€—æ—¶ï¼š1.57 æ¯«ç§’

**æ•°æ®é›†ä¸ç½‘ç»œé…ç½®**

* MNIST æ‰‹å†™æ•°å­—æ•°æ®é›†ï¼ˆ60,000 å¼ è®­ç»ƒå›¾ç‰‡ï¼‰
* ç½‘ç»œç»“æ„ï¼š784 â†’ 100 â†’ 10ï¼ˆä¸¤å±‚ç¥ç»ç½‘ç»œï¼‰
* æ‰¹å¤„ç†å¤§å°ï¼šæ¯æ‰¹ 100 å¼ å›¾ç‰‡
* å­¦ä¹ ç‡ï¼š0.1

**é¢„æµ‹å‡†ç¡®ç‡**

æµ‹è¯•é›†å‡†ç¡®ç‡ï¼š92.07%

![mnist_train](/images/ai/mnist_train.png)

## äººå·¥æ™ºèƒ½æ•ˆç‡æ—¶é—´çº¿

| æ—¶é—´              | äº‹ä»¶/çªç ´                       | æ ¸å¿ƒæ•ˆç‡é©å‘½               | å¯¹è®­ç»ƒæ•ˆç‡å½±å“                        |
| --------------- | --------------------------- | -------------------- | ------------------------------ |
| **1943-1958**   | ç¥ç»ç½‘ç»œé›å½¢ï¼ˆMcCulloch-Pittsã€æ„ŸçŸ¥æœºï¼‰ | ç®€å•çº¿æ€§è®¡ç®—               | å¯ä»¥åšç®€å•åˆ†ç±»ï¼Œä½†ä»…èƒ½å¤„ç†å°è§„æ¨¡é—®é¢˜             |
| **1960s-1970s** | æ„ŸçŸ¥æœºå‘å±• & XORé—®é¢˜æå‡º             | ç®—æ³•é™åˆ¶æ˜¾ç°               | è®­ç»ƒæ·±å±‚ç½‘ç»œä¸å¯è¡Œï¼Œæ•ˆç‡æä½                 |
| **1986**        | åå‘ä¼ æ’­ç®—æ³•ï¼ˆRumelhart ç­‰ï¼‰         | é“¾å¼æ³•åˆ™è‡ªåŠ¨æ±‚æ¢¯åº¦            | æ•°å€¼å¾®åˆ† â†’ åå‘ä¼ æ’­ï¼Œè®­ç»ƒé€Ÿåº¦æé«˜æ•°ç™¾å€          |
| **1990s**       | å°å‹ç¥ç»ç½‘ç»œ & CPUè®­ç»ƒ              | è½¯ä»¶ä¼˜åŒ– + mini-batch    | è®­ç»ƒ MNIST ç­‰å°æ•°æ®é›†å¯åœ¨åˆ†é’Ÿï½å°æ—¶çº§å®Œæˆ       |
| **2006**        | æ·±åº¦ä¿¡å¿µç½‘ç»œï¼ˆHintonï¼‰              | æ— ç›‘ç£é¢„è®­ç»ƒ               | æ›´æ·±ç½‘ç»œå¯è®­ç»ƒï¼Œå‡å°‘æ¢¯åº¦æ¶ˆå¤±é—®é¢˜               |
| **2010**        | GPUç”¨äºæ·±åº¦å­¦ä¹ ï¼ˆCUDNN + CUDAï¼‰     | å¹¶è¡ŒçŸ©é˜µè®¡ç®—               | MNIST/Imagenetè®­ç»ƒé€Ÿåº¦ä»å°æ—¶ â†’ å‡ åˆ†é’Ÿ/å°æ—¶ |
| **2012**        | AlexNetèµ¢ImageNet            | GPU + ReLU + dropout | å›¾åƒè¯†åˆ«æ€§èƒ½æš´å¢ï¼Œæ·±åº¦å­¦ä¹ çœŸæ­£çˆ†å‘              |
| **2015**        | é«˜æ•ˆæ¡†æ¶ï¼ˆTensorFlow, PyTorchï¼‰   | è‡ªåŠ¨æ±‚å¯¼ + GPUä¼˜åŒ–         | è®­ç»ƒé€Ÿåº¦å¿«ï¼Œå¼€å‘æ•ˆç‡æé«˜ï¼Œæ™®é€šç ”ç©¶è€…ä¹Ÿèƒ½è®­ç»ƒæ¨¡å‹       |
| **2020**        | å¤§è¯­è¨€æ¨¡å‹ï¼ˆGPT, BERTï¼‰            | è¶…å¤§è§„æ¨¡å¹¶è¡Œ + åˆ†å¸ƒå¼è®­ç»ƒ       | å‚æ•°é‡ä»äº¿ â†’ åƒäº¿çº§ï¼Œè®­ç»ƒè€—æ—¶ä¾æ—§å¯æ§ï¼ˆç§’ â†’ å¤©ï¼‰    |
| **2023+**       | å‚ç›´/ä½æˆæœ¬å¤§æ¨¡å‹ & GPU/TPU äº‘ç«¯      | ç®—æ³• + ç¡¬ä»¶ååŒ            | ä¸ªäººå’Œå°å›¢é˜Ÿä¹Ÿèƒ½è®­ç»ƒè¡Œä¸šæ¨¡å‹ï¼Œè®­ç»ƒæ•ˆç‡è¾¾åˆ°æ–°é«˜        |


## æŸ¥çœ‹æœ¬æœºGPU

```bash

$ nvidia-smi 
Thu Feb 12 17:12:08 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 580.126.09             Driver Version: 580.126.09     CUDA Version: 13.0     |
+-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce RTX 3070        Off |   00000000:07:00.0  On |                  N/A |
| 30%   41C    P2             53W /  180W |    2555MiB /   8192MiB |      2%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A            2078      G   /usr/lib/xorg/Xorg                     1239MiB |
|    0   N/A  N/A            2213      G   /usr/bin/gnome-shell                    227MiB |
|    0   N/A  N/A            2482      G   clash-nyanpasu                            3MiB |
|    0   N/A  N/A           20456      G   ...rack-uuid=3190708988185955192        201MiB |
|    0   N/A  N/A          274485      G   /usr/share/code/code                    276MiB |
|    0   N/A  N/A          290452      G   ...x/android-studio/jbr/bin/java         21MiB |
|    0   N/A  N/A         1796518      C   /bin/python3                            506MiB |
+-----------------------------------------------------------------------------------------+

```

**ç¬¬ä¸€éƒ¨åˆ†**

* NVIDIA-SMI 580.126.09 â†’ NVIDIA ç³»ç»Ÿç®¡ç†æ¥å£ï¼ˆS.M.I.ï¼‰ç‰ˆæœ¬
* Driver Version: 580.126.09 â†’ å½“å‰ NVIDIA æ˜¾å¡é©±åŠ¨ç‰ˆæœ¬
* CUDA Version: 13.0 â†’ å½“å‰é©±åŠ¨æ”¯æŒçš„ CUDA ç‰ˆæœ¬

**ç¬¬äºŒéƒ¨åˆ†**

* GPU Name â†’ æ˜¾å¡å‹å·ï¼šRTX 3070
* Persistence-M â†’ æŒä¹…æ¨¡å¼ï¼ˆæ§åˆ¶ GPU é©±åŠ¨æ˜¯å¦æŒç»­ä¿ç•™å†…å­˜/ä¸Šä¸‹æ–‡ï¼‰
* Bus-Id â†’ PCIe æ€»çº¿ä½ç½®
* Disp.A â†’ æ˜¯å¦ç”¨äºæ˜¾ç¤ºè¾“å‡ºï¼ˆYes/Noï¼‰
* Volatile Uncorr. ECC â†’ é”™è¯¯æ ¡æ­£ç çŠ¶æ€ï¼ˆN/A è¡¨ç¤ºæ—  ECCï¼‰
* Fan â†’ é£æ‰‡è½¬é€Ÿï¼ˆ%ï¼‰
* Temp â†’ GPU æ¸©åº¦ï¼ˆâ„ƒï¼‰
* Perf â†’ å½“å‰æ€§èƒ½çŠ¶æ€ï¼ˆP0 æœ€å¿«ï¼ŒP2 çœç”µï¼‰
* Pwr:Usage/Cap â†’ å½“å‰åŠŸè€— / æœ€å¤§åŠŸè€—ï¼ˆWï¼‰
* Memory-Usage â†’ æ˜¾å­˜å ç”¨ / æ€»æ˜¾å­˜ï¼ˆMBï¼‰
* GPU-Util â†’ GPU åˆ©ç”¨ç‡ï¼ˆ%ï¼‰
* Compute M. â†’ å½“å‰è®¡ç®—æ¨¡å¼

**ç¬¬ä¸‰éƒ¨åˆ†**

* GPU â†’ ä½¿ç”¨è¯¥æ˜¾å¡ç¼–å·
* GI / CI â†’ GPU Instance / Compute Instanceï¼ˆå¤šå®ä¾‹æ¨¡å¼ï¼ŒN/A è¡¨ç¤ºæœªå¯ç”¨ï¼‰
* PID â†’ å ç”¨ GPU çš„è¿›ç¨‹ ID
* Type â†’ C = è®¡ç®— (Compute)ï¼ŒG = å›¾å½¢ (Graphics)
* Process name â†’ å ç”¨ GPU çš„ç¨‹åº
* GPU Memory Usage â†’ å ç”¨æ˜¾å­˜å¤§å°

## Pytorch GPU ç‰ˆæœ¬å®‰è£…

![pytorch_install_gpu](/images/ai/pytorch_install_gpu.png)

```bash

$ pip3 install torch torchvision --index-url https://download.pytorch.org/whl/cu130
Defaulting to user installation because normal site-packages is not writeable
Looking in indexes: https://download.pytorch.org/whl/cu130
Requirement already satisfied: torch in /home/lxg/.local/lib/python3.10/site-packages (2.6.0+cu124)
Requirement already satisfied: torchvision in /home/lxg/.local/lib/python3.10/site-packages (0.21.0+cu124)
Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/lxg/.local/lib/python3.10/site-packages (from torch) (9.1.0.70)
Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/lxg/.local/lib/python3.10/site-packages (from torch) (12.4.127)
Requirement already satisfied: sympy==1.13.1 in /home/lxg/.local/lib/python3.10/site-packages (from torch) (1.13.1)
Requirement already satisfied: triton==3.2.0 in /home/lxg/.local/lib/python3.10/site-packages (from torch) (3.2.0)
Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/lxg/.local/lib/python3.10/site-packages (from torch) (12.4.5.8)
Requirement already satisfied: typing-extensions>=4.10.0 in /home/lxg/.local/lib/python3.10/site-packages (from torch) (4.15.0)
Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/lxg/.local/lib/python3.10/site-packages (from torch) (11.2.1.3)
Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/lxg/.local/lib/python3.10/site-packages (from torch) (12.4.127)
Requirement already satisfied: filelock in /home/lxg/.local/lib/python3.10/site-packages (from torch) (3.20.3)
Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/lxg/.local/lib/python3.10/site-packages (from torch) (11.6.1.9)
Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/lxg/.local/lib/python3.10/site-packages (from torch) (10.3.5.147)
Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /home/lxg/.local/lib/python3.10/site-packages (from torch) (0.6.2)
Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/lxg/.local/lib/python3.10/site-packages (from torch) (12.4.127)
Requirement already satisfied: networkx in /home/lxg/.local/lib/python3.10/site-packages (from torch) (3.3)
Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/lxg/.local/lib/python3.10/site-packages (from torch) (12.4.127)
Requirement already satisfied: jinja2 in /home/lxg/.local/lib/python3.10/site-packages (from torch) (3.1.4)
Requirement already satisfied: fsspec in /home/lxg/.local/lib/python3.10/site-packages (from torch) (2024.6.1)
Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/lxg/.local/lib/python3.10/site-packages (from torch) (2.21.5)
Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/lxg/.local/lib/python3.10/site-packages (from torch) (12.4.127)
Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/lxg/.local/lib/python3.10/site-packages (from torch) (12.3.1.170)
Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/lxg/.local/lib/python3.10/site-packages (from sympy==1.13.1->torch) (1.3.0)
Requirement already satisfied: numpy in /home/lxg/.local/lib/python3.10/site-packages (from torchvision) (2.1.1)
Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/lxg/.local/lib/python3.10/site-packages (from torchvision) (10.4.0)
Requirement already satisfied: MarkupSafe>=2.0 in /home/lxg/.local/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)

```

**éªŒè¯pytorchæ˜¯å¦å·²ç»å®‰è£…**

```py

import torch
x = torch.rand(5, 3)
print(x)

# tensor([[0.6462, 0.3514, 0.2290],
#        [0.5519, 0.8355, 0.9929],
#        [0.0225, 0.2399, 0.3644],
#        [0.0654, 0.3948, 0.8368],
#        [0.2206, 0.8843, 0.5664]])

```

## æ”¹æˆGPU PyTorchè®­ç»ƒ

```py

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import time  # ç”¨äºè®°å½•è®­ç»ƒè€—æ—¶
from mnist_demo import load_mnist
import matplotlib.pyplot as plt

# ========== è®¾ç½® GPU è®¾å¤‡ ==========
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ä½¿ç”¨è®¾å¤‡: {device}")
print(f"GPU å‹å·: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}")
print()

# åŠ è½½ MNIST æ‰‹å†™æ•°å­—æ•°æ®é›†
# normalize=Trueï¼šåƒç´ å€¼ç¼©æ”¾åˆ° 0-1ï¼ˆä¾¿äºç¥ç»ç½‘ç»œå¤„ç†ï¼‰
# flatten=Trueï¼šæŠŠ 28x28 çš„å›¾ç‰‡å±•å¹³æˆ 784 ç»´å‘é‡
# one_hot_label=Trueï¼šæ ‡ç­¾è½¬æ¢æˆç‹¬çƒ­ç¼–ç ï¼ˆä¾‹å¦‚ 5 å˜æˆ [0,0,0,0,0,1,0,0,0,0]ï¼‰
(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, flatten=True, one_hot_label=True)

# è½¬æ¢ä¸º PyTorch å¼ é‡å¹¶ç§»åˆ° GPU
x_train = torch.from_numpy(x_train).float().to(device)
t_train = torch.from_numpy(t_train).float().to(device)
x_test = torch.from_numpy(x_test).float().to(device)
t_test = torch.from_numpy(t_test).float().to(device)

train_loss_list = []  # å­˜å‚¨æ¯æ¬¡è¿­ä»£çš„æŸå¤±å€¼ï¼Œç”¨äºç”»å›¾è§‚å¯Ÿè®­ç»ƒè¿‡ç¨‹

# ========== å®šä¹‰ä¸¤å±‚ç¥ç»ç½‘ç»œ ==========
class TwoLayerNetPyTorch(nn.Module):
    def __init__(self, input_size=784, hidden_size=100, output_size=10):
        super(TwoLayerNetPyTorch, self).__init__()
        # ç¬¬ä¸€å±‚ï¼š784 -> 100
        self.fc1 = nn.Linear(input_size, hidden_size)
        # ç¬¬äºŒå±‚ï¼š100 -> 10
        self.fc2 = nn.Linear(hidden_size, output_size)
        # æ¿€æ´»å‡½æ•°
        self.sigmoid = nn.Sigmoid()
    
    def forward(self, x):
        # ç¬¬ä¸€å±‚ + sigmoid æ¿€æ´»
        x = self.sigmoid(self.fc1(x))
        # ç¬¬äºŒå±‚ + softmaxï¼ˆåœ¨æŸå¤±å‡½æ•°ä¸­åšï¼Œè¿™é‡Œç›´æ¥è¾“å‡ºï¼‰
        x = self.fc2(x)
        return x

# åˆå§‹åŒ–ç½‘ç»œ
network = TwoLayerNetPyTorch(input_size=784, hidden_size=100, output_size=10).to(device)

# ========== è®­ç»ƒè¶…å‚æ•°è®¾ç½® ==========
iters_num = 10000  # æ€»å…±è®­ç»ƒå¤šå°‘æ­¥
train_size = x_train.shape[0]  # è®­ç»ƒé›†ä¸­æœ‰å¤šå°‘å¼ å›¾ç‰‡
batch_size = 100  # æ¯æ¬¡è®­ç»ƒç”¨å¤šå°‘å¼ å›¾ç‰‡
learning_rate = 0.1  # å­¦ä¹ ç‡

# å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
criterion = nn.CrossEntropyLoss()  # äº¤å‰ç†µæŸå¤±
optimizer = optim.SGD(network.parameters(), lr=learning_rate)

# ========== è®°å½•è®­ç»ƒå¼€å§‹æ—¶é—´ ==========
start_time = time.time()
print("=" * 50)
print("å¼€å§‹åœ¨ GPU ä¸Šè®­ç»ƒç¥ç»ç½‘ç»œ...")
print("=" * 50)

# ========== å¼€å§‹è®­ç»ƒå¾ªç¯ ==========
for i in range(iters_num):
    # ä»è®­ç»ƒé›†ä¸­éšæœºæŠ½å– batch_size å¼ å›¾ç‰‡
    batch_mask = np.random.choice(train_size, batch_size)
    x_batch = x_train[batch_mask]
    t_batch = t_train[batch_mask]
    
    # å‰å‘ä¼ æ’­
    outputs = network(x_batch)
    
    # è®¡ç®—æŸå¤±ï¼ˆt_batch éœ€è¦è½¬æ¢ä¸ºç±»åˆ«ç´¢å¼•ï¼‰
    # t_batch æ˜¯ç‹¬çƒ­ç¼–ç ï¼Œéœ€è¦è½¬æ¢ä¸ºç±»åˆ«ç´¢å¼•
    t_batch_indices = torch.argmax(t_batch, dim=1)
    loss = criterion(outputs, t_batch_indices)
    
    # åå‘ä¼ æ’­å’Œå‚æ•°æ›´æ–°
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    # è®°å½•æŸå¤±å€¼
    train_loss_list.append(loss.item())
    
    # æ¯ 1000 æ­¥æ‰“å°ä¸€æ¬¡è¿›åº¦
    if i % 1000 == 0:
        print(f"è®­ç»ƒè¿›åº¦: ç¬¬ {i} æ­¥ï¼Œå½“å‰æŸå¤±å€¼: {loss.item():.4f}")

# ========== è®¡ç®—è®­ç»ƒè€—æ—¶ ==========
end_time = time.time()
elapsed_time = end_time - start_time
minutes = int(elapsed_time // 60)
seconds = int(elapsed_time % 60)

# ========== è¯„ä¼°æ¨¡å‹ ==========
network.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
with torch.no_grad():
    # æµ‹è¯•é›†å‡†ç¡®ç‡
    test_outputs = network(x_test)
    test_preds = torch.argmax(test_outputs, dim=1)
    test_labels = torch.argmax(t_test, dim=1)
    test_accuracy = (test_preds == test_labels).float().mean().item()
    print(f"\næµ‹è¯•é›†å‡†ç¡®ç‡: {test_accuracy:.4f} ({test_accuracy * 100:.2f}%)")

# ========== è®¾ç½® matplotlib æ”¯æŒä¸­æ–‡æ˜¾ç¤º ==========
plt.rcParams['font.sans-serif'] = ['SimHei', 'Noto Sans CJK SC', 'WenQuanYi Zen Hei', 'DejaVu Sans', 'Liberation Sans']
plt.rcParams['axes.unicode_minus'] = False

# ========== ç»˜åˆ¶æŸå¤±å‡½æ•°æ›²çº¿ ==========
plt.figure(figsize=(12, 6))

# ç»˜åˆ¶æŸå¤±å‡½æ•°å€¼æ¨ç§»æ›²çº¿
plt.plot(train_loss_list, linewidth=0.5)

# è®¾ç½®å›¾è¡¨æ ‡é¢˜å’Œæ ‡ç­¾
plt.title("GPU è®­ç»ƒè¿‡ç¨‹ä¸­çš„æŸå¤±å‡½æ•°å€¼å˜åŒ–", fontsize=14, fontweight='bold')
plt.xlabel("è¿­ä»£æ¬¡æ•°ï¼ˆæ¯æ¬¡å¤„ç† 100 å¼ å›¾ç‰‡ï¼‰", fontsize=12)
plt.ylabel("æŸå¤±å‡½æ•°å€¼ï¼ˆäº¤å‰ç†µï¼‰", fontsize=12)

# æ·»åŠ ç½‘æ ¼çº¿
plt.grid(True, alpha=0.3)

# æ·»åŠ æ–‡æœ¬è¯´æ˜
final_loss = train_loss_list[-1]
info_text = f'æœ€ç»ˆæŸå¤±å€¼: {final_loss:.4f}\nè®­ç»ƒè€—æ—¶: {minutes}åˆ† {seconds}ç§’\nè®¾å¤‡: GPU'
plt.text(len(train_loss_list) * 0.65, max(train_loss_list) * 0.85, 
         info_text, 
         fontsize=11, bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.7))

# ç´§å¯†å¸ƒå±€
plt.tight_layout()

# æ˜¾ç¤ºå›¾è¡¨
plt.show()

# ========== æ‰“å°è®­ç»ƒç»Ÿè®¡ä¿¡æ¯ ==========
print(f"\n{'=' * 50}")
print(f"         GPU è®­ç»ƒå®Œæˆï¼")
print(f"{'=' * 50}")
print(f"æ€»è¿­ä»£æ¬¡æ•°: {iters_num}")
print(f"åˆå§‹æŸå¤±å€¼: {train_loss_list[0]:.4f}")
print(f"æœ€ç»ˆæŸå¤±å€¼: {train_loss_list[-1]:.4f}")
print(f"æŸå¤±ä¸‹é™å¹…åº¦: {(train_loss_list[0] - train_loss_list[-1]):.4f}")
print(f"\nâ±ï¸  è®­ç»ƒè€—æ—¶: {minutes} åˆ† {seconds} ç§’ï¼ˆå…± {elapsed_time:.2f} ç§’ï¼‰")
print(f"å¹³å‡æ¯æ­¥è€—æ—¶: {elapsed_time / iters_num * 1000:.2f} æ¯«ç§’")
print(f"{'=' * 50}")


```

![train_mnist_gpu_pytorch](/images/ai/train_mnist_gpu_pytorch.png)

## Pytorch å’Œ CUDA åœ¨äººå·¥æ™ºèƒ½å¼€å‘ä¸­çš„ä½ç½®

| å±‚çº§                                        | ä½œç”¨                            | å…¸å‹å·¥å…·                                        |
| ----------------------------------------- | ----------------------------- | ------------------------------------------- |
| **ç¡¬ä»¶å±‚ï¼ˆCompute Hardwareï¼‰**                 | æä¾›åŸå§‹ç®—åŠ›ï¼ŒçŸ©é˜µè¿ç®—ã€å·ç§¯è¿ç®—ç­‰å¯†é›†è®¡ç®—         | CPUã€GPUï¼ˆNVIDIA RTXï¼‰ã€TPUã€FPGA                |
| **é©±åŠ¨ä¸åº•å±‚åŠ é€Ÿå±‚ï¼ˆHardware Acceleration / APIï¼‰** | å°†é«˜å±‚ç®—æ³•ç¿»è¯‘ä¸ºç¡¬ä»¶å¯æ‰§è¡ŒæŒ‡ä»¤ï¼Œå®ç°å¹¶è¡Œè®¡ç®—å’ŒåŠ é€Ÿ     | CUDAï¼ˆNVIDIA GPUï¼‰ã€cuDNNã€ROCmï¼ˆAMD GPUï¼‰ã€OpenCL |
| **æ·±åº¦å­¦ä¹ æ¡†æ¶å±‚ï¼ˆFramework / APIï¼‰**              | æä¾›é«˜å±‚æ¥å£ï¼šå®šä¹‰ç½‘ç»œã€è®¡ç®—æ¢¯åº¦ã€ä¼˜åŒ–å‚æ•°ï¼Œéšè—åº•å±‚å¤æ‚æ€§ | PyTorchã€TensorFlowã€JAX                      |

**å¼€æºå¯¹æ¯”**

| å¯¹æ¯”é¡¹  | PyTorch                   | CUDA                   |
| ---- | ------------------------- | ---------------------- |
| æ˜¯å¦å¼€æº | âœ… æ˜¯                       | âŒ å¦                    |
| ä½œç”¨å±‚çº§ | æ¡†æ¶å±‚ï¼ˆPython APIã€æ¨¡å‹å®šä¹‰ã€è®­ç»ƒé€»è¾‘ï¼‰ | é©±åŠ¨ + GPU æ ¸å¿ƒè®¡ç®—          |
| å¯ä¿®æ”¹æ€§ | å®Œå…¨å¯ä»¥ä¿®æ”¹ã€ä¼˜åŒ–ã€æ·»åŠ ç®—å­            | ä¸èƒ½ä¿®æ”¹ï¼Œåªèƒ½è°ƒç”¨æ¥å£            |
| åŸå›    | ç¤¾åŒºé©±åŠ¨ã€ç ”ç©¶å…±äº«ã€åŠ é€Ÿç”Ÿæ€å‘å±•          | GPU ç¡¬ä»¶ä¸“åˆ©å’Œå•†ä¸šæ ¸å¿ƒï¼Œé—­æºä¿æŠ¤å•†ä¸šåˆ©ç›Š |

CUDA æ˜¯ NVIDIA çš„æ ¸å¿ƒç«äº‰åŠ›å’Œç”Ÿæ€é”å®šå·¥å…·ï¼ŒçŸ­æœŸå†…æ— æ³•å¤åˆ¶ã€‚åä¸ºé€‰æ‹©é—­ç¯è‡ªç ”ç”Ÿæ€ï¼ŒAMDé€‰æ‹©å¼€æºè·¨å¹³å°ç­–ç•¥ï¼Œè€Œä¸æ˜¯é€ ä¸€ä¸ªâ€œå±äºè‡ªå·±å¤§ CUDAâ€

## Google

| æ¡†æ¶             | CPU | NVIDIA GPU     | AMD GPU  | Google TPU    |
| -------------- | --- | -------------- | -------- | ------------- |
| **PyTorch**    | âœ…   | âœ… (CUDA/cuDNN) | âœ… (ROCm) | âœ… (torch_xla) |
| **TensorFlow** | âœ…   | âœ… (CUDA/cuDNN) | âœ… (ROCm) | âœ… (åŸç”Ÿæ”¯æŒ TPU)  |

Google èƒ½åšåˆ°å…¨æ ˆé—­ç¯ï¼Œå› ä¸ºå®ƒä»ç¡¬ä»¶ã€ç¼–è¯‘å™¨åˆ°æ¡†æ¶å’Œäº‘å…¨è‡ªç ”ï¼Œæ¯ä¸€å±‚éƒ½å¯ä»¥ååŒä¼˜åŒ–ï¼›è€Œ NVIDIAã€AMD æˆ–å›½äº§ GPU éƒ½æ˜¯ç¡¬ä»¶ä¸ºæ ¸å¿ƒï¼Œç”Ÿæ€ã€æ¡†æ¶å’Œäº‘æœåŠ¡ä¾èµ–å¤–éƒ¨èµ„æºï¼Œå› æ­¤æ— æ³•å®Œå…¨è„±ç¦»ç¬¬ä¸‰æ–¹ï¼ˆå¦‚ CUDAï¼‰å½¢æˆé—­ç¯

**é˜¿é‡Œå¦‚æœæœ‰äº†è‡ªå·±çš„TPUï¼Œæ˜¯å¦ç±»ä¼¼äºgoogleçš„æ¨¡å¼äº†**

| å±‚é¢         | Google TPU æ¨¡å¼                          | é˜¿é‡Œå®ç°æ¡ä»¶                                      |
| ---------- | -------------------------------------- | ------------------------------------------- |
| **ç¡¬ä»¶**     | TPU Matrix Coreï¼Œè‡ªç ” AI åŠ é€Ÿå™¨ï¼Œé«˜åº¦ä¼˜åŒ–çŸ©é˜µè®¡ç®—å’Œå·ç§¯  | é˜¿é‡Œéœ€è¦å®Œå…¨è‡ªä¸»è®¾è®¡çš„ AI åŠ é€ŸèŠ¯ç‰‡ï¼ˆçŸ©é˜µä¹˜/å·ç§¯/æ³¨æ„åŠ›ä¼˜åŒ–ï¼‰           |
| **ç¼–è¯‘å™¨/é©±åŠ¨** | XLAï¼šTensorFlow è¿ç®—ç¼–è¯‘æˆ TPU æŒ‡ä»¤ï¼Œè‡ªåŠ¨è°ƒåº¦å†…å­˜/çŸ©é˜µ  | é˜¿é‡Œéœ€è¦ç±»ä¼¼ XLA çš„ JIT ç¼–è¯‘å™¨ï¼Œå°†æ¡†æ¶è¿ç®—æ˜ å°„åˆ°è‡ªç ” TPU         |
| **æ·±åº¦å­¦ä¹ æ¡†æ¶** | TensorFlow ä¸ TPU å®Œå…¨èåˆï¼Œæ¡†æ¶ API å¯ç›´æ¥è°ƒç”¨ TPU | é˜¿é‡Œéœ€è¦æ¡†æ¶ï¼ˆPaddle/MindSporeï¼‰åŸç”Ÿæ”¯æŒ TPU APIï¼Œç®—å­é«˜åº¦ä¼˜åŒ– |
| **äº‘/é›†ç¾¤**   | Google Cloud TPU + åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒ             | é˜¿é‡Œéœ€è¦è‡ªå®¶äº‘ï¼ˆé˜¿é‡Œäº‘ï¼‰æ”¯æŒ TPU è°ƒåº¦ã€åˆ†å¸ƒå¼è®­ç»ƒå’Œæ¨ç†              |





















