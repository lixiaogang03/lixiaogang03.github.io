---
layout:     post
title:      AI 法律大模型
subtitle:   律师
date:       2026-01-21
author:     LXG
header-img: img/post-bg-coffee.jpeg
catalog: true
tags:
    - AI
---

## 需求

1. 案件材料、证据、当事人信息不可外传
2. 可审计、可解释
3. 中文法律文本能力很重要

## 推荐的大模型

| 场景          | 模型               |
| ----------- | ---------------- |
| 日常法律问答、文书辅助 | Qwen2.5-7B       |
| 法条比对、复杂分析   | Qwen2.5-14B      |
| 律所“核心中枢”    | Qwen2.5-32B（成本高） |

## 硬件方案-个人使用

**硬件配置**

```markdown

CPU: AMD 7950X / Intel Xeon W
内存: 128GB DDR5（越大越好）
GPU: RTX 4090 24GB（1 张）
存储:
  - NVMe 2TB（模型 + 索引）
  - SATA 8TB（案件材料）

```

**能跑的大模型**

1. Qwen2.5-7B（满血 FP16）
2. Qwen2.5-14B（INT8 / AWQ）
3. 同时支持 RAG

💰 成本：≈ 3～4 万人民币
🟢 性价比最高

## 关键架构：不是“模型”，而是RAG

```markdown

案件资料 / 判决书 / 法条
        ↓
   本地向量库（FAISS / Milvus）
        ↓
    RAG 检索
        ↓
   本地大模型推理


```

**个人律师离线大模型的最优解是：RTX 4090 + Qwen2.5-14B + 本地 RAG**

**适合的场景**

| 功能               | 是否适合 |
| ---------------- | ---- |
| 文书起草（合同/起诉状/答辩状） | ✅    |
| 案件摘要与重点提取        | ✅    |
| 法条说明与逻辑推理        | ✅    |
| RAG 结合本地向量检索     | ✅    |
| 结构化输出（表格/JSON）   | ✅    |
| 单人问答、辅助分析        | ✅    |


## 硬件方案-3–10 人同时使用

📍 并发推理压力
📍 内存与显存竞争
📍 RAG 检索同时请求
📍 文档库容量更大
📍 访问控制与审计要求

**硬件配置**

```markdown

CPU: 1 × AMD EPYC 7402P / 7950X
内存: 256–384GB
GPU: 2 × NVIDIA RTX 4090
存储: 4TB NVMe + 10TB SATA
网络: 千兆或 10G 内网

```


| GPU    | 并发用户     | 模型                   |
| ------ | -------- | -------------------- |
| 1×4090 | 2–3 用户   | Qwen2.5-7B           |
| 2×4090 | 6–10 用户  | Qwen2.5-14B          |
| 4×4090 | 15–30 用户 | Qwen2.5-14B / 部分 32B |

## 费用

**单用户费用**

| 部件           | 典型规格                  | 估计成本（人民币）           |
| ------------ | --------------------- | ------------------- |
| **GPU**      | NVIDIA RTX 4090（24GB） | ~¥12,000-¥15,000    |
| **CPU**      | AMD Ryzen 9 7950X     | ~¥3,000-¥5,000      |
| **内存**       | 128GB DDR5            | ~¥3,000-¥5,000      |
| **存储**       | 2TB NVMe + 4-8TB SATA | ~¥2,000-¥4,000      |
| **主板**       | 支持大内存/单卡扩展            | ~¥1,000-¥2,000      |
| **电源/机箱/散热** | 配套稳定散热系统              | ~¥2,000-¥3,000      |
| **总计（大致）**   |                       | **¥23,000-¥34,000** |

**多用户费用**

| 部件           | 典型规格                    | 成本范围（人民币）           |
| ------------ | ----------------------- | ------------------- |
| **GPU ×2**   | 2 × RTX 4090（24GB ×2）   | ~¥24,000-¥30,000    |
| **CPU**      | AMD EPYC/High-End Ryzen | ~¥5,000-¥10,000     |
| **内存**       | 256-384GB               | ~¥6,000-¥12,000     |
| **存储**       | 4TB NVMe + 10-20TB 数据存储 | ~¥4,000-¥8,000      |
| **主板**       | 支持双卡 & 多内存插槽            | ~¥1,500-¥3,000      |
| **电源/散热/机箱** | 大电源 + 机箱                | ~¥3,000-¥5,000      |
| **总计（大致）**   |                         | **¥43,000-¥68,000** |

## 单用户双GPU卡的情况呢

**物理显存限制**

1. 单卡 4090 显存 24GB
2. 两卡 4090 显存 不能简单叠加成 48GB（因为 NVLink 只支持数据传输，不是完全共享显存）
3. Qwen2.5‑14B 本身量化后 INT8 可在一张 24GB 4090 上运行
4. 更大模型如 32B、34B、70B：
   * 单卡 24GB 显存 远远不够
   * 双卡 24GB×2 在一般消费级显卡上 也不够直接放完整模型
   * 需要 模型并行或拆分推理

**单用户使用，一般情况下 单卡 4090 就够了**

* 显存需求：
  * 14B 模型 INT8 / FP16 量化 → 一张 4090（24GB）完全够
* 响应速度：
  * 单用户同时只有一个会话，不会出现显存抢占问题

* RAG + Embedding：
  * Embedding / 向量检索可以交给 CPU 或 GPU 并行轻量任务
  * 不需要第二张 GPU 专门处理


## 更高级模型的硬件需求

| 模型               | 参数规模  | 未量化显存需求     | 量化显存需求（INT8/低精度） | 可用显卡建议                  | 备注                             |
| ---------------- | ----- | ----------- | ---------------- | ----------------------- | ------------------------------ |
| **Qwen2.5‑0.5B** | ~0.5B | ~ <2 GB     | ~1 GB            | 入门 GPU（≥4 GB）           | 可用于轻量任务、CPU 也能跑  |
| **Qwen2.5‑1.5B** | ~1.5B | ~~3 GB      | ~2 GB            | 中端 GPU（6 GB+）           | 能做基本问答          |
| **Qwen2.5‑3B**   | ~3B   | ~6 GB       | ~4 GB            | ≥RTX 2060/显存8 GB        | 中等模型        |
| **Qwen2.5‑7B**   | ~7B   | ~14 GB      | ~8–11 GB         | RTX 3090/4090（≥24 GB）   | 单卡可用；量化更省显存    |
| **Qwen2.5‑14B**  | ~14B  | ~28 GB      | ~13–18 GB        | RTX 4090（24 GB） *可量化运行* | 主流线上/本地模型     |
| **Qwen2.5‑32B**  | ~32B  | ~60–70 GB   | ~20–32 GB        | 多卡或 48–80 GB GPU        | 量化后可在更强显卡上运行   |
| **Qwen2.5‑72B**  | ~72B  | ~134–140 GB | ~41–71 GB        | 特殊多卡或 A100/H100         | 需要专业集群或模型    |

## 中国大陆能购买到的算力强的GPU

| GPU 类型                        | 官方可买    | 适合模型规模         | 备注             |
| ----------------------------- | ------- | -------------- | -------------- |
| **RTX 4090 24GB**             | ✅       | 7B–14B 及量化较大模型 | 性价比最高          |
| **RTX A5000 24GB**            | 🟡      | 7B–14B / 专业场景  | 工作站更稳定         |
| **RTX 5080 16GB 系列**          | ⚠       | 7B 以下或轻量任务     | 市场情况不稳定        |
| **RTX PRO 4000/5000 24‑48GB** | ⚠       | 中到大模型尝试        | 可通过企业渠道        |
| **RTX 5090 32GB**             | ❌（正规渠道） | 量化 32B 及更大模型   | 限制较严，不建议盼望正规供应 |

## Qwen2.5-14B 对比专业律师模型

### 📍 Qwen2.5‑14B（通用大模型）
✔ **优势**：
- 综合语言理解、逻辑推理、文本生成能力较好  
- 本地量化 / 部署生态丰富  
- 处理大量通用法律语言任务可靠  
✔ **局限**：
- 在细粒度合同条款精确解析/专业判例推理上可能不如专业模型

**适合**：
合同草拟、法律问答、长文档摘要、流程性任务


### 📍 InternLM‑Law（中文法律专用）
✔ 针对中文法律场景训练，包括:
- 标准法律问题
- 复杂现实案件解析
✔ 在 LawBench 等法律基准测试中表现优异，甚至超过 GPT‑4 的部分任务
✔ 属于在通用 + 法律数据微调的融合优势

**适合**：
法律咨询、判例分析、专业条款解释、法律战略建议


### 📍 LaWGPT（中文法律大模型）
✔ 基于通用模型如 LLaMA/ChatGLM，扩充大量中文法律语料
✔ 加入专有法律词表和司法考试数据精调，提高法律问答准确性
✔ 适合中国律师日常使用场景

**适合**：
中文法律谈话、法律文书草拟、司法解释说明


### 📍 Unilaw‑R1（法律推理模型）
✔ 专注法律推理训练 + 强 CoT（Chain‑of‑Thought）策略
✔ 在某些法律推理任务上显著超过同规模通用模型

**适合**：
复杂法律逻辑推理、法律决策 support、判例理据分析

### 📍 专业 Legal‑LLM（如 Legal‑BERT / Contracts‑BERT 等）
✔ 不是大参数 LLM，但在 contract classification / contract understanding 等任务上常超越通用模型
✔ 通常与大型模型一起在 pipeline 中使用（如先分类再生成）

**适合**：
合同结构分析、法条标注、分类/实体识别类任务













